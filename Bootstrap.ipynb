{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from algorithms.fcm import FCM\n",
    "from algorithms.gk import GK\n",
    "from algorithms.nPyFCM import nPyFMC\n",
    "from algorithms.nPyGK import nPyGK\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "\n",
    "def fuzzy_indices(X, membership, centers, m=2):\n",
    "        # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[:,k] ** m) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(centers.shape[0])\n",
    "    ])\n",
    "    # Separation for XBI\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[:, k] ** m) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    return pc, xbi, fsi\n",
    "\n",
    "\n",
    "def hard_indices(X, labels, true_labels):\n",
    "    n_samples = X.shape[0]\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2 or -1 in unique_labels:\n",
    "        return [np.nan] * 9\n",
    "\n",
    "    centers = np.array([X[labels == label].mean(axis=0) for label in unique_labels])\n",
    "    membership = np.zeros((n_samples, len(unique_labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        membership[i, np.where(unique_labels == label)[0][0]] = 1\n",
    "\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[:, k] ** 2) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(len(unique_labels))\n",
    "    ])\n",
    "\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[:, k] ** 2) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    ss = silhouette_score(X, labels)\n",
    "    ars = adjusted_rand_score(true_labels, labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, labels)\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_labels, labels)\n",
    "\n",
    "    return pc, xbi, fsi, ss, ars, ami, h, c, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2444d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_clustering(X, y, n_clusters=3, n_iter=100):\n",
    "    results = {name: [] for name in ['FCM']}            \n",
    "\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        idx = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[idx]\n",
    "        y_sample = y[idx]\n",
    "\n",
    "        # FCM\n",
    "        fmc = FCM(n_clusters=3,m=2,max_iter=300)\n",
    "        fcm_centers = fmc.fit(X_sample)\n",
    "        fcm_labels = fmc.predict(X_sample)\n",
    "        pc, xbi, fsi = fuzzy_indices(X_sample, fcm_labels, fcm_centers)\n",
    "        labels = np.argmax(fcm_labels,axis=-1)\n",
    "        ss = silhouette_score(X_sample, labels)\n",
    "        ars = adjusted_rand_score(y_sample, labels)\n",
    "        ami = adjusted_mutual_info_score(y_sample, labels)\n",
    "        h, c, v = homogeneity_completeness_v_measure(y_sample, labels)\n",
    "        results['FCM'].append((pc, xbi, fsi, ss, ars, ami, h, c, v))\n",
    "    # Summarize results\n",
    "    summary = {}\n",
    "    for method, scores in results.items():\n",
    "        scores = np.array(scores, dtype=np.float64)\n",
    "        summary[method] = {\n",
    "            'mean': np.nanmean(scores, axis=0).round(4),\n",
    "            'std': np.nanstd(scores, axis=0).round(4),\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "# Run the evaluation\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "result = bootstrap_clustering(X, y, n_clusters=3, n_iter=100)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other than fuzzy c mean  for iris dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans,DBSCAN, SpectralClustering, AgglomerativeClustering\n",
    "from algorithms.gk import GK\n",
    "from algorithms.nPyFCM import nPyFCM\n",
    "from algorithms.nPyGK import nPyGK\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "\n",
    "def fuzzy_indices(X, membership, centers, m=2):\n",
    "        # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[k,:] ** m) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(centers.shape[0])\n",
    "    ])\n",
    "    # Separation for XBI\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[k,:] ** m) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    return pc, xbi, fsi\n",
    "\n",
    "\n",
    "def hard_indices(X, labels, true_labels):\n",
    "    n_samples = X.shape[0]\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2 or -1 in unique_labels:\n",
    "        return [np.nan] * 9\n",
    "\n",
    "    centers = np.array([X[labels == label].mean(axis=0) for label in unique_labels])\n",
    "    membership = np.zeros((n_samples, len(unique_labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        membership[i, np.where(unique_labels == label)[0][0]] = 1\n",
    "\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[:, k] ** 2) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(len(unique_labels))\n",
    "    ])\n",
    "\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[:, k] ** 2) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    ss = silhouette_score(X, labels)\n",
    "    ars = adjusted_rand_score(true_labels, labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, labels)\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_labels, labels)\n",
    "\n",
    "    return pc, xbi, fsi, ss, ars, ami, h, c, v\n",
    "\n",
    "\n",
    "\n",
    "def bootstrap_clustering(X, y, n_clusters=3, n_iter=100):\n",
    "    results = {name: [] for name in ['GK','DBSCAN', 'Spectral', 'Hierarchical','nPyFCM','nPyGK']}            \n",
    "\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        idx = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[idx]\n",
    "        y_sample = y[idx]\n",
    "\n",
    "        # GK\n",
    "        gk = GK(n_clusters=3,m=2,max_iter=300)\n",
    "        gk_centers = gk.fit(X_sample)\n",
    "        gk_labels = gk.predict(X_sample)\n",
    "        pc, xbi, fsi = fuzzy_indices(X_sample, gk_labels, gk_centers)\n",
    "        labels = np.argmax(gk_labels,axis=0)\n",
    "        ss = silhouette_score(X_sample, labels)\n",
    "        ars = adjusted_rand_score(y_sample, labels)\n",
    "        ami = adjusted_mutual_info_score(y_sample, labels)\n",
    "        h, c, v = homogeneity_completeness_v_measure(y_sample, labels)\n",
    "        results['GK'].append((pc, xbi, fsi, ss, ars, ami, h, c, v))\n",
    "\n",
    "        # nPyFMC\n",
    "        nPyfcm = nPyFCM(n_clusters=3,m=2,max_iter=100,n_pyth=3,alpha=0.514286)\n",
    "        nPyfcm_centers = nPyfcm.fit(X_sample)\n",
    "        nPyfcm_labels = nPyfcm.predict(X_sample)\n",
    "        pc, xbi, fsi = fuzzy_indices(X_sample, nPyfcm_labels, nPyfcm_centers)\n",
    "        labels = np.argmax(nPyfcm_labels,axis=0)\n",
    "        ss = silhouette_score(X_sample, labels)\n",
    "        ars = adjusted_rand_score(y_sample, labels)\n",
    "        ami = adjusted_mutual_info_score(y_sample, labels)\n",
    "        h, c, v = homogeneity_completeness_v_measure(y_sample, labels)\n",
    "        results['nPyFCM'].append((pc, xbi, fsi, ss, ars, ami, h, c, v))\n",
    "\n",
    "        # nPyGK\n",
    "        nPygk = nPyGK(n_clusters=3,m=2,max_iter=100,n_pyth=5,alpha=1.8)\n",
    "        nPygk_centers = nPygk.fit(X_sample)\n",
    "        nPygk_labels = nPygk.predict(X_sample)\n",
    "        pc, xbi, fsi = fuzzy_indices(X_sample, nPygk_labels, nPygk_centers)\n",
    "        labels = np.argmax(nPygk_labels,axis=0)\n",
    "        ss = silhouette_score(X_sample, labels)\n",
    "        ars = adjusted_rand_score(y_sample, labels)\n",
    "        ami = adjusted_mutual_info_score(y_sample, labels)\n",
    "        h, c, v = homogeneity_completeness_v_measure(y_sample, labels)\n",
    "        results['nPyGK'].append((pc, xbi, fsi, ss, ars, ami, h, c, v))\n",
    "\n",
    "        # DBSCAN\n",
    "        db = DBSCAN(eps=0.6, min_samples=4).fit(X_sample)\n",
    "        labels = db.fit_predict(X)\n",
    "        # Filter noise\n",
    "        mask = labels != -1\n",
    "        \n",
    "        X_filtered = X_sample[mask]\n",
    "        labels_filtered = labels[mask]\n",
    "        y_filtered = y_sample[mask]\n",
    "        results['DBSCAN'].append(hard_indices(X_filtered, labels_filtered, y_filtered))\n",
    "\n",
    "        # Spectral Clustering\n",
    "        sc = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors').fit(X_sample)\n",
    "        results['Spectral'].append(hard_indices(X_sample, sc.labels_, y_sample))\n",
    "\n",
    "        # Hierarchical\n",
    "        ac = AgglomerativeClustering(n_clusters=n_clusters).fit(X_sample)\n",
    "        results['Hierarchical'].append(hard_indices(X_sample, ac.labels_, y_sample))\n",
    "    # Summarize results\n",
    "    summary = {}\n",
    "    for method, scores in results.items():\n",
    "        scores = np.array(scores, dtype=np.float64)\n",
    "        np.set_printoptions(precision=4)\n",
    "        summary[method] = {\n",
    "            'mean': np.nanmean(scores, axis=0),\n",
    "            'std': np.nanstd(scores, axis=0),\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "# Run the evaluation\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "result = bootstrap_clustering(X, y, n_clusters=3, n_iter=100)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912cfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def hard_indices(X, labels, true_labels):\n",
    "    n_samples = X.shape[0]\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2 or -1 in unique_labels:\n",
    "        return [np.nan] * 9\n",
    "\n",
    "    centers = np.array([X[labels == label].mean(axis=0) for label in unique_labels])\n",
    "    membership = np.zeros((n_samples, len(unique_labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        membership[i, np.where(unique_labels == label)[0][0]] = 1\n",
    "\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[:, k] ** 2) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(len(unique_labels))\n",
    "    ])\n",
    "\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[:, k] ** 2) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    ss = silhouette_score(X, labels)\n",
    "    ars = adjusted_rand_score(true_labels, labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, labels)\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_labels, labels)\n",
    "\n",
    "    return pc, xbi, fsi, ss, ars, ami, h, c, v\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "\n",
    "\n",
    "def bootstrap_clustering(X, y, n_clusters=3, n_iter=100):\n",
    "    results = {name: [] for name in ['DBSCAN']}        \n",
    "\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        idx = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[idx]\n",
    "        y_sample = y[idx]\n",
    "        # DBSCAN\n",
    "    db = DBSCAN(eps=0.6, min_samples=4).fit(X_sample)\n",
    "    labels = db.fit_predict(X)\n",
    "\n",
    "    # Filter noise\n",
    "    mask = labels != -1\n",
    "    X_filtered = X_sample[mask]\n",
    "    labels_filtered = labels[mask]\n",
    "    y_filtered = y_sample[mask]\n",
    "    results['DBSCAN'].append(hard_indices(X_filtered, labels_filtered, y_filtered))\n",
    "\n",
    "    summary = {}\n",
    "    for method, scores in results.items():\n",
    "        scores = np.array(scores, dtype=np.float64)\n",
    "        summary[method] = {\n",
    "            'mean': np.nanmean(scores, axis=0).round(4),\n",
    "            'std': np.nanstd(scores, axis=0).round(4),\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_sample = scaler.fit_transform(X_data)\n",
    "\n",
    "result = bootstrap_clustering(X, y, n_clusters=3, n_iter=100)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other than fuzzy c mean  for iris dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from algorithms.nPyGK import nPyGK\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "\n",
    "def fuzzy_indices(X, membership, centers, m=2):\n",
    "        # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[k,:] ** m) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(centers.shape[0])\n",
    "    ])\n",
    "    # Separation for XBI\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[k,:] ** m) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    return pc, xbi, fsi\n",
    "\n",
    "\n",
    "def hard_indices(X, labels, true_labels):\n",
    "    n_samples = X.shape[0]\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2 or -1 in unique_labels:\n",
    "        return [np.nan] * 9\n",
    "\n",
    "    centers = np.array([X[labels == label].mean(axis=0) for label in unique_labels])\n",
    "    membership = np.zeros((n_samples, len(unique_labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        membership[i, np.where(unique_labels == label)[0][0]] = 1\n",
    "\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[:, k] ** 2) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(len(unique_labels))\n",
    "    ])\n",
    "\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[:, k] ** 2) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    ss = silhouette_score(X, labels)\n",
    "    ars = adjusted_rand_score(true_labels, labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, labels)\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_labels, labels)\n",
    "\n",
    "    return pc, xbi, fsi, ss, ars, ami, h, c, v\n",
    "\n",
    "\n",
    "\n",
    "def bootstrap_clustering(X, y, n_clusters=3, n_iter=100):\n",
    "    results = {name: [] for name in ['GK','DBSCAN', 'Spectral', 'Hierarchical','nPyFCM','nPyGK']}            \n",
    "\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        idx = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[idx]\n",
    "        y_sample = y[idx]\n",
    "\n",
    "        # nPyGK\n",
    "        nPygk = nPyGK(n_clusters=3,m=2,max_iter=100,n_pyth=5,alpha=1.8)\n",
    "        nPygk_centers = nPygk.fit(X_sample)\n",
    "        nPygk_labels = nPygk.predict(X_sample)\n",
    "        pc, xbi, fsi = fuzzy_indices(X_sample, nPygk_labels, nPygk_centers)\n",
    "        labels = np.argmax(nPygk_labels,axis=0)\n",
    "        ss = silhouette_score(X_sample, labels)\n",
    "        ars = adjusted_rand_score(y_sample, labels)\n",
    "        ami = adjusted_mutual_info_score(y_sample, labels)\n",
    "        h, c, v = homogeneity_completeness_v_measure(y_sample, labels)\n",
    "        results['nPyGK'].append((pc, xbi, fsi, ss, ars, ami, h, c, v))\n",
    "\n",
    "    # Summarize results\n",
    "    summary = {}\n",
    "    for method, scores in results.items():\n",
    "        scores = np.array(scores, dtype=np.float64)\n",
    "        np.set_printoptions(precision=4)\n",
    "        summary[method] = {\n",
    "            'mean': np.nanmean(scores, axis=0),\n",
    "            'std': np.nanstd(scores, axis=0),\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "def generate_dataframe(X,true_label_encoded, number_of_clusters, m, MAX_ITER):\n",
    "    data = []\n",
    "    # Loop through n_pyth and alpha values and get the results\n",
    "    for n_pyth in np.arange(1, 6):\n",
    "        for alpha in np.linspace(0.1, n_pyth, 50):\n",
    "            nPygk = nPyGK(n_clusters=number_of_clusters, m=m, max_iter=MAX_ITER, n_pyth=n_pyth, alpha=alpha)\n",
    "            cluster_centers = nPygk.fit(X)\n",
    "            predicted_labels = nPygk.predict(X)\n",
    "            metrices = calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded)\n",
    "            # Add n_pyth, alpha, and dictionary values (a, b, c, ..., g) to the data list\n",
    "            data.append({'n_pyth': n_pyth, 'alpha': alpha, **metrices})\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_max_indices_for_n_pyth(df, unique_output_dir,m):\n",
    "\n",
    "    # Automatically identify metric columns (excluding n_pyth and alpha)\n",
    "    df = df.dropna()\n",
    "    metric_columns = [col for col in df.columns if col not in [\"n_pyth\", \"alpha\"]]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Loop through unique values of n_pyth\n",
    "    for n_pyth in df[\"n_pyth\"].unique():\n",
    "        filtered_df = df[df[\"n_pyth\"] == n_pyth]\n",
    "\n",
    "        # Find the max or min value and corresponding alpha for each metric column\n",
    "        for col in metric_columns:\n",
    "            if col in [\"xie beni index\",\"fukuyama sugeno index\"]:\n",
    "                # For Xie-Beni index, find the minimum value\n",
    "                min_row = filtered_df.loc[filtered_df[col].dropna().idxmin()]\n",
    "                results.append({\n",
    "                    \"n_pyth\": n_pyth,\n",
    "                    \"metric\": col,\n",
    "                    \"value\": min_row[col],\n",
    "                    \"alpha\": min_row[\"alpha\"]\n",
    "                })\n",
    "            else:\n",
    "                # For other metrics, find the maximum value\n",
    "                max_row = filtered_df.loc[filtered_df[col].dropna().idxmax()]\n",
    "                results.append({\n",
    "                    \"n_pyth\": n_pyth,\n",
    "                    \"metric\": col,\n",
    "                    \"value\": max_row[col],\n",
    "                    \"alpha\": max_row[\"alpha\"]\n",
    "                })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Generate the LaTeX table from the entire DataFrame\n",
    "    latex_table = results_df.to_latex(\n",
    "        index=False,  # Do not include row indices\n",
    "        caption=\"Best Values of Different Indices for Values of n_pyth and Alpha\",  \n",
    "        label=\"tab 1: Max indices\"  # Optional label for referencing the table\n",
    "    )\n",
    "\n",
    "    # Save the LaTeX table to a file\n",
    "    with open(f\"{unique_output_dir}/Clustering_Metrics_Table_maximum.tex\", \"w\") as file:\n",
    "        file.write(latex_table)\n",
    "\n",
    "    print(f\"LaTeX table saved to {unique_output_dir}/Clustering_Metrics_Table_maximum.tex\")\n",
    "\n",
    "# Run the evaluation\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "result = bootstrap_clustering(X, y, n_clusters=3, n_iter=100)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from algorithms.nPyGK import nPyGK\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "def fuzzy_indices(X, membership, centers, m=2.1):\n",
    "        # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[k,:] ** m) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(centers.shape[0])\n",
    "    ])\n",
    "    # Separation for XBI\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[k,:] ** m) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    return pc, xbi, fsi\n",
    "\n",
    "def bootstrap_clustering_analysis(X, y, n_clusters=3, m=2, n_iter=100, n_pyth_range=range(1,6), alpha_points=25):\n",
    "    \"\"\"\n",
    "    Perform bootstrap clustering analysis with varying n_pyth and alpha values.\n",
    "    \n",
    "    Returns:\n",
    "        - Raw results from all bootstrap iterations\n",
    "        - Summary statistics (mean, std) for each parameter combination\n",
    "    \"\"\"\n",
    "    raw_results = []\n",
    "    \n",
    "    for _ in tqdm(range(n_iter), desc=\"Bootstrap iterations\"):\n",
    "        idx = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[idx]\n",
    "        y_sample = y[idx]\n",
    "        \n",
    "        for n_pyth in n_pyth_range:\n",
    "            alpha_values = np.linspace(0.2, n_pyth, alpha_points)\n",
    "            \n",
    "            for alpha in alpha_values:\n",
    "                try:\n",
    "                    nPygk = nPyGK(n_clusters=n_clusters, m=m, max_iter=100, \n",
    "                                 n_pyth=n_pyth, alpha=alpha)\n",
    "                    centers = nPygk.fit(X_sample)\n",
    "                    predicted_labels = nPygk.predict(X_sample)\n",
    "                    \n",
    "                    # Calculate all metrics\n",
    "                    pc, xbi, fsi = fuzzy_indices(X_sample, predicted_labels, centers)\n",
    "                    labels = np.argmax(predicted_labels, axis=0)\n",
    "                    ss = silhouette_score(X_sample, labels)\n",
    "                    ars = adjusted_rand_score(y_sample, labels)\n",
    "                    ami = adjusted_mutual_info_score(y_sample, labels)\n",
    "                    h, c, v = homogeneity_completeness_v_measure(y_sample, labels)\n",
    "                    \n",
    "                    raw_results.append({\n",
    "                        'n_pyth': n_pyth,\n",
    "                        'alpha': alpha,\n",
    "                        'PC': pc,\n",
    "                        'XBI': xbi,\n",
    "                        'FSI': fsi,\n",
    "                        'Silhouette': ss,\n",
    "                        'ARS': ars,\n",
    "                        'AMI': ami,\n",
    "                        'Homogeneity': h,\n",
    "                        'Completeness': c,\n",
    "                        'V-measure': v,\n",
    "                        'iteration': _\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed for n_pyth={n_pyth}, alpha={alpha}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Verify we have the expected columns\n",
    "    required_columns = {'n_pyth', 'alpha'}\n",
    "    if not required_columns.issubset(raw_df.columns):\n",
    "        missing = required_columns - set(raw_df.columns)\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    raw_df = pd.DataFrame(raw_results)\n",
    "    # Now safe to groupby\n",
    "    summary_df = raw_df.groupby(['n_pyth', 'alpha']).agg(['mean', 'std']).reset_index()\n",
    "    summary_df.columns = ['_'.join(col).strip() for col in summary_df.columns.values]\n",
    "    summary_df = summary_df.rename(columns={'n_pyth_': 'n_pyth', 'alpha_': 'alpha'})\n",
    "\n",
    "def find_best_parameters(summary_df, output_dir=None):\n",
    "    \"\"\"\n",
    "    For each n_pyth, find the alpha that gives the best average value for each metric.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with best parameters for each n_pyth and metric\n",
    "    \"\"\"\n",
    "    # Define optimization direction for each metric\n",
    "    metric_info = {\n",
    "        'PC': {'direction': 'max', 'full_name': 'Partition Coefficient'},\n",
    "        'XBI': {'direction': 'min', 'full_name': 'Xie-Beni Index'},\n",
    "        'FSI': {'direction': 'min', 'full_name': 'Fukuyama-Sugeno Index'},\n",
    "        'Silhouette': {'direction': 'max', 'full_name': 'Silhouette Score'},\n",
    "        'ARS': {'direction': 'max', 'full_name': 'Adjusted Rand Score'},\n",
    "        'AMI': {'direction': 'max', 'full_name': 'Adjusted Mutual Info'},\n",
    "        'Homogeneity': {'direction': 'max', 'full_name': 'Homogeneity'},\n",
    "        'Completeness': {'direction': 'max', 'full_name': 'Completeness'},\n",
    "        'V-measure': {'direction': 'max', 'full_name': 'V-measure'}\n",
    "    }\n",
    "    \n",
    "    best_params = []\n",
    "    \n",
    "    for n_pyth in summary_df['n_pyth'].unique():\n",
    "        n_pyth_subset = summary_df[summary_df['n_pyth'] == n_pyth]\n",
    "        \n",
    "        for metric, info in metric_info.items():\n",
    "            mean_col = f'{metric}_mean'\n",
    "            std_col = f'{metric}_std'\n",
    "            \n",
    "            if info['direction'] == 'max':\n",
    "                best_row = n_pyth_subset.loc[n_pyth_subset[mean_col].idxmax()]\n",
    "            else:\n",
    "                best_row = n_pyth_subset.loc[n_pyth_subset[mean_col].idxmin()]\n",
    "            \n",
    "            best_params.append({\n",
    "                'n_pyth': n_pyth,\n",
    "                'metric': info['full_name'],\n",
    "                'best_alpha': best_row['alpha'],\n",
    "                'mean_value': best_row[mean_col],\n",
    "                'std_value': best_row[std_col],\n",
    "                'optimization': info['direction']\n",
    "            })\n",
    "    \n",
    "    best_df = pd.DataFrame(best_params)\n",
    "    \n",
    "    if output_dir:\n",
    "        # Save results\n",
    "        best_df.to_csv(f\"{output_dir}/best_parameters.csv\", index=False)\n",
    "        \n",
    "        # Generate LaTeX table\n",
    "        latex_table = best_df.to_latex(\n",
    "            index=False,\n",
    "            caption=\"Best parameters from bootstrap analysis\",\n",
    "            label=\"tab:best_params\",\n",
    "            float_format=\"%.4f\",\n",
    "            column_format='l' * (len(best_df.columns) + 1)\n",
    "        )\n",
    "        \n",
    "        with open(f\"{output_dir}/best_parameters.tex\", \"w\") as f:\n",
    "            f.write(latex_table)\n",
    "    \n",
    "    return best_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(\"Data/iris.csv\")\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = LabelEncoder().fit_transform(df.iloc[:, -1].values)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Run bootstrap analysis\n",
    "    raw_results, summary_stats = bootstrap_clustering_analysis(\n",
    "        X, y,\n",
    "        n_clusters=3,\n",
    "        m=2,\n",
    "        n_iter=100,\n",
    "        n_pyth_range=range(1,6),\n",
    "        alpha_points=50\n",
    "    )\n",
    "    \n",
    "    # Find best parameters\n",
    "    best_params = find_best_parameters(summary_stats, \"results\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Best parameters for each n_pyth:\")\n",
    "    print(best_params)\n",
    "    \n",
    "    # Additional analysis: Best overall parameters\n",
    "    print(\"\\nBest overall parameters across all n_pyth values:\")\n",
    "    for metric in ['PC', 'Silhouette', 'ARS', 'V-measure']:  # Example metrics to highlight\n",
    "        if metric in ['XBI', 'FSI']:\n",
    "            best_overall = best_params[best_params['metric'].str.contains(metric)].nsmallest(1, 'mean_value')\n",
    "        else:\n",
    "            best_overall = best_params[best_params['metric'].str.contains(metric)].nlargest(1, 'mean_value')\n",
    "        print(best_overall.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dfa0d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_pyth=1, alpha=0.2000: 100%|██████████| 100/100 [00:00<00:00, 111.87it/s]\n",
      "n_pyth=1, alpha=0.2163: 100%|██████████| 100/100 [00:00<00:00, 114.64it/s]\n",
      "n_pyth=1, alpha=0.2327: 100%|██████████| 100/100 [00:00<00:00, 111.85it/s]\n",
      "n_pyth=1, alpha=0.2490: 100%|██████████| 100/100 [00:00<00:00, 106.90it/s]\n",
      "n_pyth=1, alpha=0.2653: 100%|██████████| 100/100 [00:00<00:00, 101.52it/s]\n",
      "n_pyth=1, alpha=0.2816: 100%|██████████| 100/100 [00:01<00:00, 99.69it/s]\n",
      "n_pyth=1, alpha=0.2980: 100%|██████████| 100/100 [00:01<00:00, 93.92it/s]\n",
      "n_pyth=1, alpha=0.3143: 100%|██████████| 100/100 [00:01<00:00, 91.85it/s]\n",
      "n_pyth=1, alpha=0.3306: 100%|██████████| 100/100 [00:01<00:00, 89.44it/s]\n",
      "n_pyth=1, alpha=0.3469: 100%|██████████| 100/100 [00:01<00:00, 95.32it/s]\n",
      "n_pyth=1, alpha=0.3633: 100%|██████████| 100/100 [00:01<00:00, 88.44it/s]\n",
      "n_pyth=1, alpha=0.3796: 100%|██████████| 100/100 [00:01<00:00, 87.35it/s]\n",
      "n_pyth=1, alpha=0.3959: 100%|██████████| 100/100 [00:01<00:00, 69.89it/s]\n",
      "n_pyth=1, alpha=0.4122: 100%|██████████| 100/100 [00:02<00:00, 42.30it/s]\n",
      "n_pyth=1, alpha=0.4286: 100%|██████████| 100/100 [00:02<00:00, 39.16it/s]\n",
      "n_pyth=1, alpha=0.4449: 100%|██████████| 100/100 [00:02<00:00, 35.89it/s]\n",
      "n_pyth=1, alpha=0.4612: 100%|██████████| 100/100 [00:03<00:00, 32.59it/s]\n",
      "n_pyth=1, alpha=0.4776: 100%|██████████| 100/100 [00:03<00:00, 29.18it/s]\n",
      "n_pyth=1, alpha=0.4939: 100%|██████████| 100/100 [00:03<00:00, 25.20it/s]\n",
      "n_pyth=1, alpha=0.5102: 100%|██████████| 100/100 [00:04<00:00, 22.06it/s]\n",
      "n_pyth=1, alpha=0.5265: 100%|██████████| 100/100 [00:06<00:00, 15.96it/s]\n",
      "n_pyth=1, alpha=0.5429: 100%|██████████| 100/100 [00:07<00:00, 12.59it/s]\n",
      "n_pyth=1, alpha=0.5592: 100%|██████████| 100/100 [00:12<00:00,  8.17it/s]\n",
      "n_pyth=1, alpha=0.5755: 100%|██████████| 100/100 [00:12<00:00,  8.04it/s]\n",
      "n_pyth=1, alpha=0.5918: 100%|██████████| 100/100 [00:12<00:00,  7.88it/s]\n",
      "n_pyth=1, alpha=0.6082: 100%|██████████| 100/100 [00:12<00:00,  7.97it/s]\n",
      "n_pyth=1, alpha=0.6245: 100%|██████████| 100/100 [00:11<00:00,  8.96it/s]\n",
      "n_pyth=1, alpha=0.6408: 100%|██████████| 100/100 [00:09<00:00, 10.37it/s]\n",
      "n_pyth=1, alpha=0.6571: 100%|██████████| 100/100 [00:08<00:00, 12.44it/s]\n",
      "n_pyth=1, alpha=0.6735: 100%|██████████| 100/100 [00:07<00:00, 12.59it/s]\n",
      "n_pyth=1, alpha=0.6898: 100%|██████████| 100/100 [00:06<00:00, 14.44it/s]\n",
      "n_pyth=1, alpha=0.7061: 100%|██████████| 100/100 [00:06<00:00, 15.32it/s]\n",
      "n_pyth=1, alpha=0.7224: 100%|██████████| 100/100 [00:06<00:00, 16.62it/s]\n",
      "n_pyth=1, alpha=0.7388: 100%|██████████| 100/100 [00:05<00:00, 16.92it/s]\n",
      "n_pyth=1, alpha=0.7551: 100%|██████████| 100/100 [00:05<00:00, 16.87it/s]\n",
      "n_pyth=1, alpha=0.7714: 100%|██████████| 100/100 [00:05<00:00, 17.34it/s]\n",
      "n_pyth=1, alpha=0.7878: 100%|██████████| 100/100 [00:05<00:00, 17.94it/s]\n",
      "n_pyth=1, alpha=0.8041: 100%|██████████| 100/100 [00:05<00:00, 18.69it/s]\n",
      "n_pyth=1, alpha=0.8204: 100%|██████████| 100/100 [00:05<00:00, 17.75it/s]\n",
      "n_pyth=1, alpha=0.8367: 100%|██████████| 100/100 [00:05<00:00, 18.33it/s]\n",
      "n_pyth=1, alpha=0.8531: 100%|██████████| 100/100 [00:05<00:00, 17.64it/s]\n",
      "n_pyth=1, alpha=0.8694: 100%|██████████| 100/100 [00:05<00:00, 17.63it/s]\n",
      "n_pyth=1, alpha=0.8857: 100%|██████████| 100/100 [00:05<00:00, 18.66it/s]\n",
      "n_pyth=1, alpha=0.9020: 100%|██████████| 100/100 [00:05<00:00, 18.18it/s]\n",
      "n_pyth=1, alpha=0.9184: 100%|██████████| 100/100 [00:05<00:00, 18.26it/s]\n",
      "n_pyth=1, alpha=0.9347: 100%|██████████| 100/100 [00:05<00:00, 17.84it/s]\n",
      "n_pyth=1, alpha=0.9510: 100%|██████████| 100/100 [00:05<00:00, 18.35it/s]\n",
      "n_pyth=1, alpha=0.9673: 100%|██████████| 100/100 [00:05<00:00, 18.12it/s]\n",
      "n_pyth=1, alpha=0.9837: 100%|██████████| 100/100 [00:05<00:00, 17.65it/s]\n",
      "n_pyth=1, alpha=1.0000: 100%|██████████| 100/100 [00:05<00:00, 18.46it/s]\n",
      "n_pyth=2, alpha=0.2000: 100%|██████████| 100/100 [00:00<00:00, 270.45it/s]\n",
      "n_pyth=2, alpha=0.2367: 100%|██████████| 100/100 [00:01<00:00, 71.24it/s]\n",
      "n_pyth=2, alpha=0.2735: 100%|██████████| 100/100 [00:01<00:00, 71.94it/s]\n",
      "n_pyth=2, alpha=0.3102: 100%|██████████| 100/100 [00:01<00:00, 65.37it/s]\n",
      "n_pyth=2, alpha=0.3469: 100%|██████████| 100/100 [00:01<00:00, 62.75it/s]\n",
      "n_pyth=2, alpha=0.3837: 100%|██████████| 100/100 [00:01<00:00, 63.60it/s]\n",
      "n_pyth=2, alpha=0.4204: 100%|██████████| 100/100 [00:01<00:00, 58.22it/s]\n",
      "n_pyth=2, alpha=0.4571: 100%|██████████| 100/100 [00:01<00:00, 53.02it/s]\n",
      "n_pyth=2, alpha=0.4939: 100%|██████████| 100/100 [00:01<00:00, 55.33it/s]\n",
      "n_pyth=2, alpha=0.5306: 100%|██████████| 100/100 [00:01<00:00, 52.98it/s]\n",
      "n_pyth=2, alpha=0.5673: 100%|██████████| 100/100 [00:02<00:00, 49.97it/s]\n",
      "n_pyth=2, alpha=0.6041: 100%|██████████| 100/100 [00:02<00:00, 47.10it/s]\n",
      "n_pyth=2, alpha=0.6408: 100%|██████████| 100/100 [00:02<00:00, 44.05it/s]\n",
      "n_pyth=2, alpha=0.6776: 100%|██████████| 100/100 [00:02<00:00, 40.86it/s]\n",
      "n_pyth=2, alpha=0.7143: 100%|██████████| 100/100 [00:02<00:00, 37.37it/s]\n",
      "n_pyth=2, alpha=0.7510: 100%|██████████| 100/100 [00:02<00:00, 33.50it/s]\n",
      "n_pyth=2, alpha=0.7878: 100%|██████████| 100/100 [00:03<00:00, 29.44it/s]\n",
      "n_pyth=2, alpha=0.8245: 100%|██████████| 100/100 [00:03<00:00, 26.94it/s]\n",
      "n_pyth=2, alpha=0.8612: 100%|██████████| 100/100 [00:04<00:00, 22.11it/s]\n",
      "n_pyth=2, alpha=0.8980: 100%|██████████| 100/100 [00:05<00:00, 17.79it/s]\n",
      "n_pyth=2, alpha=0.9347: 100%|██████████| 100/100 [00:07<00:00, 14.07it/s]\n",
      "n_pyth=2, alpha=0.9714: 100%|██████████| 100/100 [00:11<00:00,  8.77it/s]\n",
      "n_pyth=2, alpha=1.0082: 100%|██████████| 100/100 [00:12<00:00,  7.73it/s]\n",
      "n_pyth=2, alpha=1.0449: 100%|██████████| 100/100 [00:12<00:00,  7.71it/s]\n",
      "n_pyth=2, alpha=1.0816: 100%|██████████| 100/100 [00:13<00:00,  7.64it/s]\n",
      "n_pyth=2, alpha=1.1184: 100%|██████████| 100/100 [00:11<00:00,  8.35it/s]\n",
      "n_pyth=2, alpha=1.1551: 100%|██████████| 100/100 [00:09<00:00, 10.07it/s]\n",
      "n_pyth=2, alpha=1.1918: 100%|██████████| 100/100 [00:08<00:00, 11.87it/s]\n",
      "n_pyth=2, alpha=1.2286: 100%|██████████| 100/100 [00:07<00:00, 13.14it/s]\n",
      "n_pyth=2, alpha=1.2653: 100%|██████████| 100/100 [00:07<00:00, 13.68it/s]\n",
      "n_pyth=2, alpha=1.3020: 100%|██████████| 100/100 [00:06<00:00, 14.90it/s]\n",
      "n_pyth=2, alpha=1.3388: 100%|██████████| 100/100 [00:06<00:00, 14.94it/s]\n",
      "n_pyth=2, alpha=1.3755: 100%|██████████| 100/100 [00:06<00:00, 16.62it/s]\n",
      "n_pyth=2, alpha=1.4122: 100%|██████████| 100/100 [00:05<00:00, 17.40it/s]\n",
      "n_pyth=2, alpha=1.4490: 100%|██████████| 100/100 [00:05<00:00, 17.23it/s]\n",
      "n_pyth=2, alpha=1.4857: 100%|██████████| 100/100 [00:05<00:00, 17.68it/s]\n",
      "n_pyth=2, alpha=1.5224: 100%|██████████| 100/100 [00:05<00:00, 18.16it/s]\n",
      "n_pyth=2, alpha=1.5592: 100%|██████████| 100/100 [00:05<00:00, 17.10it/s]\n",
      "n_pyth=2, alpha=1.5959: 100%|██████████| 100/100 [00:05<00:00, 18.28it/s]\n",
      "n_pyth=2, alpha=1.6327: 100%|██████████| 100/100 [00:05<00:00, 17.33it/s]\n",
      "n_pyth=2, alpha=1.6694: 100%|██████████| 100/100 [00:06<00:00, 16.41it/s]\n",
      "n_pyth=2, alpha=1.7061: 100%|██████████| 100/100 [00:05<00:00, 17.20it/s]\n",
      "n_pyth=2, alpha=1.7429: 100%|██████████| 100/100 [00:05<00:00, 17.10it/s]\n",
      "n_pyth=2, alpha=1.7796: 100%|██████████| 100/100 [00:05<00:00, 16.79it/s]\n",
      "n_pyth=2, alpha=1.8163: 100%|██████████| 100/100 [00:05<00:00, 17.97it/s]\n",
      "n_pyth=2, alpha=1.8531: 100%|██████████| 100/100 [00:05<00:00, 17.78it/s]\n",
      "n_pyth=2, alpha=1.8898: 100%|██████████| 100/100 [00:03<00:00, 25.05it/s]\n",
      "n_pyth=2, alpha=1.9265: 100%|██████████| 100/100 [00:03<00:00, 29.63it/s]\n",
      "n_pyth=2, alpha=1.9633: 100%|██████████| 100/100 [00:03<00:00, 29.64it/s]\n",
      "n_pyth=2, alpha=2.0000: 100%|██████████| 100/100 [00:03<00:00, 31.51it/s]\n",
      "n_pyth=3, alpha=0.2000: 100%|██████████| 100/100 [00:00<00:00, 522.11it/s]\n",
      "C:\\Users\\manas\\AppData\\Local\\Temp\\ipykernel_13956\\1725061665.py:112: RuntimeWarning: Mean of empty slice\n",
      "  mean_metrics = {k + \"_mean\": np.nanmean([d[k] for d in metrics_list]) for k in metrics_list[0].keys()}\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "n_pyth=3, alpha=0.2571: 100%|██████████| 100/100 [00:00<00:00, 437.69it/s]\n",
      "n_pyth=3, alpha=0.3143: 100%|██████████| 100/100 [00:00<00:00, 104.22it/s]\n",
      "n_pyth=3, alpha=0.3714: 100%|██████████| 100/100 [00:00<00:00, 102.61it/s]\n",
      "n_pyth=3, alpha=0.4286: 100%|██████████| 100/100 [00:00<00:00, 103.45it/s]\n",
      "n_pyth=3, alpha=0.4857: 100%|██████████| 100/100 [00:01<00:00, 99.60it/s]\n",
      "n_pyth=3, alpha=0.5429: 100%|██████████| 100/100 [00:01<00:00, 93.46it/s]\n",
      "n_pyth=3, alpha=0.6000: 100%|██████████| 100/100 [00:01<00:00, 88.88it/s]\n",
      "n_pyth=3, alpha=0.6571: 100%|██████████| 100/100 [00:01<00:00, 84.39it/s]\n",
      "n_pyth=3, alpha=0.7143: 100%|██████████| 100/100 [00:01<00:00, 80.13it/s]\n",
      "n_pyth=3, alpha=0.7714: 100%|██████████| 100/100 [00:01<00:00, 74.96it/s]\n",
      "n_pyth=3, alpha=0.8286: 100%|██████████| 100/100 [00:01<00:00, 79.39it/s]\n",
      "n_pyth=3, alpha=0.8857: 100%|██████████| 100/100 [00:01<00:00, 72.19it/s]\n",
      "n_pyth=3, alpha=0.9429: 100%|██████████| 100/100 [00:01<00:00, 69.66it/s]\n",
      "n_pyth=3, alpha=1.0000: 100%|██████████| 100/100 [00:01<00:00, 64.29it/s]\n",
      "n_pyth=3, alpha=1.0571: 100%|██████████| 100/100 [00:01<00:00, 55.30it/s]\n",
      "n_pyth=3, alpha=1.1143: 100%|██████████| 100/100 [00:02<00:00, 49.24it/s]\n",
      "n_pyth=3, alpha=1.1714: 100%|██████████| 100/100 [00:02<00:00, 41.97it/s]\n",
      "n_pyth=3, alpha=1.2286: 100%|██████████| 100/100 [00:02<00:00, 34.03it/s]\n",
      "n_pyth=3, alpha=1.2857: 100%|██████████| 100/100 [00:03<00:00, 25.59it/s]\n",
      "n_pyth=3, alpha=1.3429: 100%|██████████| 100/100 [00:06<00:00, 16.66it/s]\n",
      "n_pyth=3, alpha=1.4000: 100%|██████████| 100/100 [00:07<00:00, 13.52it/s]\n",
      "n_pyth=3, alpha=1.4571: 100%|██████████| 100/100 [00:07<00:00, 13.51it/s]\n",
      "n_pyth=3, alpha=1.5143: 100%|██████████| 100/100 [00:07<00:00, 13.45it/s]\n",
      "n_pyth=3, alpha=1.5714: 100%|██████████| 100/100 [00:07<00:00, 13.91it/s]\n",
      "n_pyth=3, alpha=1.6286: 100%|██████████| 100/100 [00:06<00:00, 16.58it/s]\n",
      "n_pyth=3, alpha=1.6857: 100%|██████████| 100/100 [00:05<00:00, 18.94it/s]\n",
      "n_pyth=3, alpha=1.7429: 100%|██████████| 100/100 [00:04<00:00, 22.77it/s]\n",
      "n_pyth=3, alpha=1.8000: 100%|██████████| 100/100 [00:04<00:00, 24.87it/s]\n",
      "n_pyth=3, alpha=1.8571: 100%|██████████| 100/100 [00:03<00:00, 26.14it/s]\n",
      "n_pyth=3, alpha=1.9143: 100%|██████████| 100/100 [00:03<00:00, 26.50it/s]\n",
      "n_pyth=3, alpha=1.9714: 100%|██████████| 100/100 [00:03<00:00, 28.75it/s]\n",
      "n_pyth=3, alpha=2.0286: 100%|██████████| 100/100 [00:03<00:00, 29.10it/s]\n",
      "n_pyth=3, alpha=2.0857: 100%|██████████| 100/100 [00:03<00:00, 30.11it/s]\n",
      "n_pyth=3, alpha=2.1429: 100%|██████████| 100/100 [00:03<00:00, 30.28it/s]\n",
      "n_pyth=3, alpha=2.2000: 100%|██████████| 100/100 [00:03<00:00, 30.46it/s]\n",
      "n_pyth=3, alpha=2.2571: 100%|██████████| 100/100 [00:03<00:00, 30.26it/s]\n",
      "n_pyth=3, alpha=2.3143: 100%|██████████| 100/100 [00:03<00:00, 31.08it/s]\n",
      "n_pyth=3, alpha=2.3714: 100%|██████████| 100/100 [00:03<00:00, 31.18it/s]\n",
      "n_pyth=3, alpha=2.4286: 100%|██████████| 100/100 [00:03<00:00, 31.92it/s]\n",
      "n_pyth=3, alpha=2.4857: 100%|██████████| 100/100 [00:03<00:00, 31.79it/s]\n",
      "n_pyth=3, alpha=2.5429: 100%|██████████| 100/100 [00:03<00:00, 31.03it/s]\n",
      "n_pyth=3, alpha=2.6000: 100%|██████████| 100/100 [00:03<00:00, 31.28it/s]\n",
      "n_pyth=3, alpha=2.6571: 100%|██████████| 100/100 [00:03<00:00, 31.37it/s]\n",
      "n_pyth=3, alpha=2.7143: 100%|██████████| 100/100 [00:03<00:00, 32.19it/s]\n",
      "n_pyth=3, alpha=2.7714: 100%|██████████| 100/100 [00:03<00:00, 30.65it/s]\n",
      "n_pyth=3, alpha=2.8286: 100%|██████████| 100/100 [00:03<00:00, 30.53it/s]\n",
      "n_pyth=3, alpha=2.8857: 100%|██████████| 100/100 [00:03<00:00, 30.76it/s]\n",
      "n_pyth=3, alpha=2.9429: 100%|██████████| 100/100 [00:03<00:00, 31.36it/s]\n",
      "n_pyth=3, alpha=3.0000: 100%|██████████| 100/100 [00:03<00:00, 30.95it/s]\n",
      "n_pyth=4, alpha=0.2000: 100%|██████████| 100/100 [00:00<00:00, 579.20it/s]\n",
      "C:\\Users\\manas\\AppData\\Local\\Temp\\ipykernel_13956\\1725061665.py:112: RuntimeWarning: Mean of empty slice\n",
      "  mean_metrics = {k + \"_mean\": np.nanmean([d[k] for d in metrics_list]) for k in metrics_list[0].keys()}\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "n_pyth=4, alpha=0.2776: 100%|██████████| 100/100 [00:00<00:00, 391.98it/s]\n",
      "n_pyth=4, alpha=0.3551: 100%|██████████| 100/100 [00:00<00:00, 119.16it/s]\n",
      "n_pyth=4, alpha=0.4327: 100%|██████████| 100/100 [00:00<00:00, 114.80it/s]\n",
      "n_pyth=4, alpha=0.5102: 100%|██████████| 100/100 [00:00<00:00, 112.51it/s]\n",
      "n_pyth=4, alpha=0.5878: 100%|██████████| 100/100 [00:00<00:00, 106.55it/s]\n",
      "n_pyth=4, alpha=0.6653: 100%|██████████| 100/100 [00:01<00:00, 99.20it/s]\n",
      "n_pyth=4, alpha=0.7429: 100%|██████████| 100/100 [00:01<00:00, 97.79it/s]\n",
      "n_pyth=4, alpha=0.8204: 100%|██████████| 100/100 [00:01<00:00, 90.80it/s]\n",
      "n_pyth=4, alpha=0.8980: 100%|██████████| 100/100 [00:01<00:00, 83.30it/s]\n",
      "n_pyth=4, alpha=0.9755: 100%|██████████| 100/100 [00:01<00:00, 77.78it/s]\n",
      "n_pyth=4, alpha=1.0531: 100%|██████████| 100/100 [00:01<00:00, 72.07it/s]\n",
      "n_pyth=4, alpha=1.1306: 100%|██████████| 100/100 [00:01<00:00, 66.98it/s]\n",
      "n_pyth=4, alpha=1.2082: 100%|██████████| 100/100 [00:01<00:00, 60.31it/s]\n",
      "n_pyth=4, alpha=1.2857: 100%|██████████| 100/100 [00:01<00:00, 53.45it/s]\n",
      "n_pyth=4, alpha=1.3633: 100%|██████████| 100/100 [00:02<00:00, 47.22it/s]\n",
      "n_pyth=4, alpha=1.4408: 100%|██████████| 100/100 [00:02<00:00, 41.65it/s]\n",
      "n_pyth=4, alpha=1.5184: 100%|██████████| 100/100 [00:02<00:00, 33.83it/s]\n",
      "n_pyth=4, alpha=1.5959: 100%|██████████| 100/100 [00:03<00:00, 26.30it/s]\n",
      "n_pyth=4, alpha=1.6735: 100%|██████████| 100/100 [00:05<00:00, 18.61it/s]\n",
      "n_pyth=4, alpha=1.7510: 100%|██████████| 100/100 [00:07<00:00, 13.54it/s]\n",
      "n_pyth=4, alpha=1.8286: 100%|██████████| 100/100 [00:07<00:00, 13.39it/s]\n",
      "n_pyth=4, alpha=1.9061: 100%|██████████| 100/100 [00:07<00:00, 13.19it/s]\n",
      "n_pyth=4, alpha=1.9837: 100%|██████████| 100/100 [00:07<00:00, 13.34it/s]\n",
      "n_pyth=4, alpha=2.0612: 100%|██████████| 100/100 [00:06<00:00, 14.91it/s]\n",
      "n_pyth=4, alpha=2.1388: 100%|██████████| 100/100 [00:05<00:00, 18.49it/s]\n",
      "n_pyth=4, alpha=2.2163: 100%|██████████| 100/100 [00:04<00:00, 20.64it/s]\n",
      "n_pyth=4, alpha=2.2939: 100%|██████████| 100/100 [00:04<00:00, 23.38it/s]\n",
      "n_pyth=4, alpha=2.3714: 100%|██████████| 100/100 [00:03<00:00, 25.32it/s]\n",
      "n_pyth=4, alpha=2.4490: 100%|██████████| 100/100 [00:03<00:00, 27.15it/s]\n",
      "n_pyth=4, alpha=2.5265: 100%|██████████| 100/100 [00:03<00:00, 26.70it/s]\n",
      "n_pyth=4, alpha=2.6041: 100%|██████████| 100/100 [00:03<00:00, 28.99it/s]\n",
      "n_pyth=4, alpha=2.6816: 100%|██████████| 100/100 [00:03<00:00, 29.95it/s]\n",
      "n_pyth=4, alpha=2.7592: 100%|██████████| 100/100 [00:03<00:00, 29.19it/s]\n",
      "n_pyth=4, alpha=2.8367: 100%|██████████| 100/100 [00:03<00:00, 31.04it/s]\n",
      "n_pyth=4, alpha=2.9143: 100%|██████████| 100/100 [00:03<00:00, 30.65it/s]\n",
      "n_pyth=4, alpha=2.9918: 100%|██████████| 100/100 [00:03<00:00, 30.58it/s]\n",
      "n_pyth=4, alpha=3.0694: 100%|██████████| 100/100 [00:03<00:00, 31.09it/s]\n",
      "n_pyth=4, alpha=3.1469: 100%|██████████| 100/100 [00:03<00:00, 31.07it/s]\n",
      "n_pyth=4, alpha=3.2245: 100%|██████████| 100/100 [00:03<00:00, 31.18it/s]\n",
      "n_pyth=4, alpha=3.3020: 100%|██████████| 100/100 [00:03<00:00, 28.65it/s]\n",
      "n_pyth=4, alpha=3.3796: 100%|██████████| 100/100 [00:03<00:00, 31.24it/s]\n",
      "n_pyth=4, alpha=3.4571: 100%|██████████| 100/100 [00:03<00:00, 30.70it/s]\n",
      "n_pyth=4, alpha=3.5347: 100%|██████████| 100/100 [00:03<00:00, 29.72it/s]\n",
      "n_pyth=4, alpha=3.6122: 100%|██████████| 100/100 [00:03<00:00, 30.27it/s]\n",
      "n_pyth=4, alpha=3.6898: 100%|██████████| 100/100 [00:03<00:00, 31.36it/s]\n",
      "n_pyth=4, alpha=3.7673: 100%|██████████| 100/100 [00:03<00:00, 31.69it/s]\n",
      "n_pyth=4, alpha=3.8449: 100%|██████████| 100/100 [00:03<00:00, 29.96it/s]\n",
      "n_pyth=4, alpha=3.9224: 100%|██████████| 100/100 [00:03<00:00, 31.16it/s]\n",
      "n_pyth=4, alpha=4.0000: 100%|██████████| 100/100 [00:03<00:00, 31.67it/s]\n",
      "n_pyth=5, alpha=0.2000: 100%|██████████| 100/100 [00:00<00:00, 660.45it/s]\n",
      "C:\\Users\\manas\\AppData\\Local\\Temp\\ipykernel_13956\\1725061665.py:112: RuntimeWarning: Mean of empty slice\n",
      "  mean_metrics = {k + \"_mean\": np.nanmean([d[k] for d in metrics_list]) for k in metrics_list[0].keys()}\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "n_pyth=5, alpha=0.2980: 100%|██████████| 100/100 [00:00<00:00, 499.93it/s]\n",
      "n_pyth=5, alpha=0.3959: 100%|██████████| 100/100 [00:00<00:00, 119.74it/s]\n",
      "n_pyth=5, alpha=0.4939: 100%|██████████| 100/100 [00:00<00:00, 116.18it/s]\n",
      "n_pyth=5, alpha=0.5918: 100%|██████████| 100/100 [00:00<00:00, 108.61it/s]\n",
      "n_pyth=5, alpha=0.6898: 100%|██████████| 100/100 [00:00<00:00, 108.47it/s]\n",
      "n_pyth=5, alpha=0.7878: 100%|██████████| 100/100 [00:01<00:00, 99.60it/s]\n",
      "n_pyth=5, alpha=0.8857: 100%|██████████| 100/100 [00:01<00:00, 93.97it/s]\n",
      "n_pyth=5, alpha=0.9837: 100%|██████████| 100/100 [00:01<00:00, 85.99it/s]\n",
      "n_pyth=5, alpha=1.0816: 100%|██████████| 100/100 [00:01<00:00, 82.16it/s]\n",
      "n_pyth=5, alpha=1.1796: 100%|██████████| 100/100 [00:01<00:00, 75.69it/s]\n",
      "n_pyth=5, alpha=1.2776: 100%|██████████| 100/100 [00:01<00:00, 67.53it/s]\n",
      "n_pyth=5, alpha=1.3755: 100%|██████████| 100/100 [00:01<00:00, 62.58it/s]\n",
      "n_pyth=5, alpha=1.4735: 100%|██████████| 100/100 [00:01<00:00, 55.60it/s]\n",
      "n_pyth=5, alpha=1.5714: 100%|██████████| 100/100 [00:02<00:00, 49.68it/s]\n",
      "n_pyth=5, alpha=1.6694: 100%|██████████| 100/100 [00:02<00:00, 44.02it/s]\n",
      "n_pyth=5, alpha=1.7673: 100%|██████████| 100/100 [00:02<00:00, 36.78it/s]\n",
      "n_pyth=5, alpha=1.8653: 100%|██████████| 100/100 [00:03<00:00, 30.26it/s]\n",
      "n_pyth=5, alpha=1.9633: 100%|██████████| 100/100 [00:04<00:00, 22.77it/s]\n",
      "n_pyth=5, alpha=2.0612: 100%|██████████| 100/100 [00:06<00:00, 15.22it/s]\n",
      "n_pyth=5, alpha=2.1592: 100%|██████████| 100/100 [00:07<00:00, 13.47it/s]\n",
      "n_pyth=5, alpha=2.2571: 100%|██████████| 100/100 [00:07<00:00, 13.52it/s]\n",
      "n_pyth=5, alpha=2.3551: 100%|██████████| 100/100 [00:07<00:00, 13.50it/s]\n",
      "n_pyth=5, alpha=2.4531: 100%|██████████| 100/100 [00:07<00:00, 13.67it/s]\n",
      "n_pyth=5, alpha=2.5510: 100%|██████████| 100/100 [00:06<00:00, 16.38it/s]\n",
      "n_pyth=5, alpha=2.6490: 100%|██████████| 100/100 [00:05<00:00, 19.31it/s]\n",
      "n_pyth=5, alpha=2.7469: 100%|██████████| 100/100 [00:04<00:00, 21.62it/s]\n",
      "n_pyth=5, alpha=2.8449: 100%|██████████| 100/100 [00:04<00:00, 23.72it/s]\n",
      "n_pyth=5, alpha=2.9429: 100%|██████████| 100/100 [00:03<00:00, 25.32it/s]\n",
      "n_pyth=5, alpha=3.0408: 100%|██████████| 100/100 [00:03<00:00, 26.51it/s]\n",
      "n_pyth=5, alpha=3.1388: 100%|██████████| 100/100 [00:03<00:00, 27.74it/s]\n",
      "n_pyth=5, alpha=3.2367: 100%|██████████| 100/100 [00:03<00:00, 28.83it/s]\n",
      "n_pyth=5, alpha=3.3347: 100%|██████████| 100/100 [00:03<00:00, 29.62it/s]\n",
      "n_pyth=5, alpha=3.4327: 100%|██████████| 100/100 [00:03<00:00, 30.76it/s]\n",
      "n_pyth=5, alpha=3.5306: 100%|██████████| 100/100 [00:03<00:00, 30.39it/s]\n",
      "n_pyth=5, alpha=3.6286: 100%|██████████| 100/100 [00:03<00:00, 31.33it/s]\n",
      "n_pyth=5, alpha=3.7265: 100%|██████████| 100/100 [00:03<00:00, 30.71it/s]\n",
      "n_pyth=5, alpha=3.8245: 100%|██████████| 100/100 [00:03<00:00, 30.94it/s]\n",
      "n_pyth=5, alpha=3.9224: 100%|██████████| 100/100 [00:03<00:00, 31.00it/s]\n",
      "n_pyth=5, alpha=4.0204: 100%|██████████| 100/100 [00:03<00:00, 31.54it/s]\n",
      "n_pyth=5, alpha=4.1184: 100%|██████████| 100/100 [00:03<00:00, 31.37it/s]\n",
      "n_pyth=5, alpha=4.2163: 100%|██████████| 100/100 [00:03<00:00, 31.69it/s]\n",
      "n_pyth=5, alpha=4.3143: 100%|██████████| 100/100 [00:03<00:00, 31.05it/s]\n",
      "n_pyth=5, alpha=4.4122: 100%|██████████| 100/100 [00:03<00:00, 31.44it/s]\n",
      "n_pyth=5, alpha=4.5102: 100%|██████████| 100/100 [00:03<00:00, 30.42it/s]\n",
      "n_pyth=5, alpha=4.6082: 100%|██████████| 100/100 [00:03<00:00, 30.41it/s]\n",
      "n_pyth=5, alpha=4.7061: 100%|██████████| 100/100 [00:03<00:00, 31.77it/s]\n",
      "n_pyth=5, alpha=4.8041: 100%|██████████| 100/100 [00:03<00:00, 30.88it/s]\n",
      "n_pyth=5, alpha=4.9020: 100%|██████████| 100/100 [00:03<00:00, 30.22it/s]\n",
      "n_pyth=5, alpha=5.0000: 100%|██████████| 100/100 [00:03<00:00, 31.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to results/Clustering_Metrics_Table_maximum_with_std.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_pyth</th>\n",
       "      <th>metric</th>\n",
       "      <th>mean_value</th>\n",
       "      <th>std_value</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>partition coefficient</td>\n",
       "      <td>2.998208</td>\n",
       "      <td>1.228290e-15</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fukuyama sugeno index</td>\n",
       "      <td>-455.384687</td>\n",
       "      <td>2.847413e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>xie beni index</td>\n",
       "      <td>0.116106</td>\n",
       "      <td>9.137613e-03</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>silhouette score</td>\n",
       "      <td>0.512140</td>\n",
       "      <td>1.692608e-02</td>\n",
       "      <td>0.510204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>adjusted rand score</td>\n",
       "      <td>0.784676</td>\n",
       "      <td>3.661461e-02</td>\n",
       "      <td>0.967347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>adjusted mutual info score</td>\n",
       "      <td>0.746542</td>\n",
       "      <td>3.316814e-02</td>\n",
       "      <td>0.967347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>homogeneity</td>\n",
       "      <td>0.747683</td>\n",
       "      <td>3.305010e-02</td>\n",
       "      <td>0.967347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>completeness</td>\n",
       "      <td>0.747271</td>\n",
       "      <td>3.306254e-02</td>\n",
       "      <td>0.967347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>v measure</td>\n",
       "      <td>0.747476</td>\n",
       "      <td>3.304616e-02</td>\n",
       "      <td>0.967347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>partition coefficient</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>fukuyama sugeno index</td>\n",
       "      <td>-463.643851</td>\n",
       "      <td>3.374521e+01</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>xie beni index</td>\n",
       "      <td>0.115288</td>\n",
       "      <td>7.712010e-03</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>silhouette score</td>\n",
       "      <td>0.512196</td>\n",
       "      <td>1.661658e-02</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>adjusted rand score</td>\n",
       "      <td>0.788733</td>\n",
       "      <td>4.162606e-02</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>adjusted mutual info score</td>\n",
       "      <td>0.749465</td>\n",
       "      <td>3.927901e-02</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>homogeneity</td>\n",
       "      <td>0.750729</td>\n",
       "      <td>3.918809e-02</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>completeness</td>\n",
       "      <td>0.750049</td>\n",
       "      <td>3.909826e-02</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>v measure</td>\n",
       "      <td>0.750388</td>\n",
       "      <td>3.913451e-02</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>partition coefficient</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.881784e-16</td>\n",
       "      <td>0.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>fukuyama sugeno index</td>\n",
       "      <td>-458.348474</td>\n",
       "      <td>2.927046e+01</td>\n",
       "      <td>2.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>xie beni index</td>\n",
       "      <td>0.115212</td>\n",
       "      <td>8.995553e-03</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>silhouette score</td>\n",
       "      <td>0.517982</td>\n",
       "      <td>1.887819e-02</td>\n",
       "      <td>1.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>adjusted rand score</td>\n",
       "      <td>0.789508</td>\n",
       "      <td>3.428353e-02</td>\n",
       "      <td>2.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>adjusted mutual info score</td>\n",
       "      <td>0.751990</td>\n",
       "      <td>3.268222e-02</td>\n",
       "      <td>2.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>homogeneity</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>3.266996e-02</td>\n",
       "      <td>2.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>completeness</td>\n",
       "      <td>0.752682</td>\n",
       "      <td>3.247452e-02</td>\n",
       "      <td>2.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>v measure</td>\n",
       "      <td>0.752904</td>\n",
       "      <td>3.256139e-02</td>\n",
       "      <td>2.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>partition coefficient</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.507013e-16</td>\n",
       "      <td>0.277551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>fukuyama sugeno index</td>\n",
       "      <td>-455.026944</td>\n",
       "      <td>2.689875e+01</td>\n",
       "      <td>3.922449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>xie beni index</td>\n",
       "      <td>0.115747</td>\n",
       "      <td>8.526561e-03</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>silhouette score</td>\n",
       "      <td>0.514268</td>\n",
       "      <td>1.937467e-02</td>\n",
       "      <td>1.673469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>adjusted rand score</td>\n",
       "      <td>0.785533</td>\n",
       "      <td>3.994503e-02</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>adjusted mutual info score</td>\n",
       "      <td>0.746561</td>\n",
       "      <td>3.698485e-02</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>homogeneity</td>\n",
       "      <td>0.747750</td>\n",
       "      <td>3.688708e-02</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>completeness</td>\n",
       "      <td>0.747241</td>\n",
       "      <td>3.682939e-02</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>v measure</td>\n",
       "      <td>0.747495</td>\n",
       "      <td>3.684865e-02</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5</td>\n",
       "      <td>partition coefficient</td>\n",
       "      <td>2.999998</td>\n",
       "      <td>5.235735e-16</td>\n",
       "      <td>0.395918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>fukuyama sugeno index</td>\n",
       "      <td>-460.353362</td>\n",
       "      <td>3.402448e+01</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5</td>\n",
       "      <td>xie beni index</td>\n",
       "      <td>0.115278</td>\n",
       "      <td>9.006826e-03</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5</td>\n",
       "      <td>silhouette score</td>\n",
       "      <td>0.513672</td>\n",
       "      <td>1.927079e-02</td>\n",
       "      <td>2.061224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5</td>\n",
       "      <td>adjusted rand score</td>\n",
       "      <td>0.783723</td>\n",
       "      <td>4.207188e-02</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5</td>\n",
       "      <td>adjusted mutual info score</td>\n",
       "      <td>0.745551</td>\n",
       "      <td>3.784793e-02</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5</td>\n",
       "      <td>homogeneity</td>\n",
       "      <td>0.746705</td>\n",
       "      <td>3.754062e-02</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5</td>\n",
       "      <td>completeness</td>\n",
       "      <td>0.746274</td>\n",
       "      <td>3.789494e-02</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>v measure</td>\n",
       "      <td>0.746488</td>\n",
       "      <td>3.770832e-02</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_pyth                      metric  mean_value     std_value     alpha\n",
       "0        1       partition coefficient    2.998208  1.228290e-15  0.200000\n",
       "1        1       fukuyama sugeno index -455.384687  2.847413e+01  1.000000\n",
       "2        1              xie beni index    0.116106  9.137613e-03  1.000000\n",
       "3        1            silhouette score    0.512140  1.692608e-02  0.510204\n",
       "4        1         adjusted rand score    0.784676  3.661461e-02  0.967347\n",
       "5        1  adjusted mutual info score    0.746542  3.316814e-02  0.967347\n",
       "6        1                 homogeneity    0.747683  3.305010e-02  0.967347\n",
       "7        1                completeness    0.747271  3.306254e-02  0.967347\n",
       "8        1                   v measure    0.747476  3.304616e-02  0.967347\n",
       "9        2       partition coefficient    3.000000  0.000000e+00  0.200000\n",
       "10       2       fukuyama sugeno index -463.643851  3.374521e+01  2.000000\n",
       "11       2              xie beni index    0.115288  7.712010e-03  2.000000\n",
       "12       2            silhouette score    0.512196  1.661658e-02  0.971429\n",
       "13       2         adjusted rand score    0.788733  4.162606e-02  2.000000\n",
       "14       2  adjusted mutual info score    0.749465  3.927901e-02  2.000000\n",
       "15       2                 homogeneity    0.750729  3.918809e-02  2.000000\n",
       "16       2                completeness    0.750049  3.909826e-02  2.000000\n",
       "17       2                   v measure    0.750388  3.913451e-02  2.000000\n",
       "18       3       partition coefficient    3.000000  8.881784e-16  0.257143\n",
       "19       3       fukuyama sugeno index -458.348474  2.927046e+01  2.942857\n",
       "20       3              xie beni index    0.115212  8.995553e-03  3.000000\n",
       "21       3            silhouette score    0.517982  1.887819e-02  1.342857\n",
       "22       3         adjusted rand score    0.789508  3.428353e-02  2.714286\n",
       "23       3  adjusted mutual info score    0.751990  3.268222e-02  2.714286\n",
       "24       3                 homogeneity    0.753129  3.266996e-02  2.714286\n",
       "25       3                completeness    0.752682  3.247452e-02  2.714286\n",
       "26       3                   v measure    0.752904  3.256139e-02  2.714286\n",
       "27       4       partition coefficient    3.000000  4.507013e-16  0.277551\n",
       "28       4       fukuyama sugeno index -455.026944  2.689875e+01  3.922449\n",
       "29       4              xie beni index    0.115747  8.526561e-03  4.000000\n",
       "30       4            silhouette score    0.514268  1.937467e-02  1.673469\n",
       "31       4         adjusted rand score    0.785533  3.994503e-02  4.000000\n",
       "32       4  adjusted mutual info score    0.746561  3.698485e-02  4.000000\n",
       "33       4                 homogeneity    0.747750  3.688708e-02  4.000000\n",
       "34       4                completeness    0.747241  3.682939e-02  4.000000\n",
       "35       4                   v measure    0.747495  3.684865e-02  4.000000\n",
       "36       5       partition coefficient    2.999998  5.235735e-16  0.395918\n",
       "37       5       fukuyama sugeno index -460.353362  3.402448e+01  5.000000\n",
       "38       5              xie beni index    0.115278  9.006826e-03  5.000000\n",
       "39       5            silhouette score    0.513672  1.927079e-02  2.061224\n",
       "40       5         adjusted rand score    0.783723  4.207188e-02  5.000000\n",
       "41       5  adjusted mutual info score    0.745551  3.784793e-02  5.000000\n",
       "42       5                 homogeneity    0.746705  3.754062e-02  5.000000\n",
       "43       5                completeness    0.746274  3.789494e-02  5.000000\n",
       "44       5                   v measure    0.746488  3.770832e-02  5.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "# from algorithms.nPyGK import nPyGK\n",
    "from algorithms.nPyGK import nPyGK\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_preprocess_data(input_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    X = df.iloc[:, :-1].values  # Features\n",
    "    true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, true_labels_encoded\n",
    "\n",
    "def calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded):\n",
    "    # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(predicted_labels**2) / n_samples\n",
    "    \n",
    "    \n",
    "    cluster_dists = [\n",
    "        np.sum((predicted_labels[k,:]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    # Minimum separation distance (between-cluster dispersion)\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Xie-Beni Index\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "    xb = np.minimum(xb,1)\n",
    "\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(predicted_labels[k,:]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Fukuyama-Sugeno Index\n",
    "    se = compactness - separation_fs   \n",
    "    \n",
    "    predicted_labels = np.argmax(predicted_labels,axis=0)\n",
    "    if len(np.unique(predicted_labels)) > 1:\n",
    "        ss = silhouette_score(X,predicted_labels)\n",
    "        ars = adjusted_rand_score(predicted_labels, true_label_encoded)\n",
    "        amis = adjusted_mutual_info_score(predicted_labels, true_label_encoded)\n",
    "        hh, cc, vv = homogeneity_completeness_v_measure(predicted_labels, true_label_encoded)\n",
    "    else:\n",
    "        ss = np.nan\n",
    "        ars = np.nan\n",
    "        amis = np.nan\n",
    "        hh, cc, vv = np.nan,np.nan,np.nan\n",
    "    return {\n",
    "        \"partition coefficient\": pc,\n",
    "        \"fukuyama sugeno index\": se,\n",
    "        \"xie beni index\": xb,\n",
    "        \"silhouette score\": ss,\n",
    "        \"adjusted rand score\": ars,\n",
    "        \"adjusted mutual info score\": amis,\n",
    "        \"homogeneity\": hh,\n",
    "        \"completeness\": cc,\n",
    "        \"v measure\": vv,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_dataframe(X, true_label_encoded, number_of_clusters, m, MAX_ITER, n_iter=100):\n",
    "    all_data = []\n",
    "\n",
    "    # Loop through n_pyth and alpha values\n",
    "    for n_pyth in np.arange(1, 6):\n",
    "        for alpha in np.linspace(0.2, n_pyth, 50):\n",
    "            metrics_list = []\n",
    "            for _ in tqdm(range(n_iter), desc=f\"n_pyth={n_pyth}, alpha={alpha:.4f}\"):\n",
    "                idx = np.random.choice(len(X), len(X), replace=True)\n",
    "                X_sample = X[idx]\n",
    "                y_sample = true_label_encoded[idx]\n",
    "\n",
    "                nPygk = nPyGK(n_clusters=number_of_clusters, m=m, max_iter=MAX_ITER, n_pyth=n_pyth, alpha=alpha)\n",
    "                cluster_centers = nPygk.fit(X_sample)\n",
    "                predicted_labels = nPygk.predict(X_sample)\n",
    "\n",
    "                metrics = calculate_indices(X_sample, predicted_labels, cluster_centers, y_sample)\n",
    "                metrics_list.append(metrics)\n",
    "\n",
    "            # Compute mean and std for all metrics over the 100 iterations\n",
    "            mean_metrics = {k + \"_mean\": np.nanmean([d[k] for d in metrics_list]) for k in metrics_list[0].keys()}\n",
    "            std_metrics = {k + \"_std\": np.nanstd([d[k] for d in metrics_list]) for k in metrics_list[0].keys()}\n",
    "\n",
    "            # Append row with means and stds for this (n_pyth, alpha) pair\n",
    "            all_data.append({'n_pyth': n_pyth, 'alpha': alpha, **mean_metrics, **std_metrics})\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def find_max_indices_for_n_pyth(df, unique_output_dir, m):\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Identify only mean metric columns (exclude std columns)\n",
    "    metric_columns = [col for col in df.columns if col.endswith(\"_mean\")]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for n_pyth in df[\"n_pyth\"].unique():\n",
    "        filtered_df = df[df[\"n_pyth\"] == n_pyth]\n",
    "\n",
    "        for col in metric_columns:\n",
    "            metric_name = col.replace(\"_mean\", \"\")\n",
    "            std_col = metric_name + \"_std\"\n",
    "\n",
    "            if metric_name in [\"xie beni index\", \"fukuyama sugeno index\"]:\n",
    "                # For these, we want to find the minimum mean\n",
    "                best_row = filtered_df.loc[filtered_df[col].idxmin()]\n",
    "            else:\n",
    "                # For others, we want the maximum mean\n",
    "                best_row = filtered_df.loc[filtered_df[col].idxmax()]\n",
    "\n",
    "            results.append({\n",
    "                \"n_pyth\": n_pyth,\n",
    "                \"metric\": metric_name,\n",
    "                \"mean_value\": best_row[col],\n",
    "                \"std_value\": best_row[std_col],\n",
    "                \"alpha\": best_row[\"alpha\"]\n",
    "            })\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save LaTeX table\n",
    "    latex_table = results_df.to_latex(\n",
    "        index=False,\n",
    "        float_format=\"%.4f\",\n",
    "        caption=\"Best Mean Values and Corresponding Standard Deviations for Each Metric and n_pyth\",\n",
    "        label=\"tab:max_indices_with_std\"\n",
    "    )\n",
    "\n",
    "    with open(f\"{unique_output_dir}/Clustering_Metrics_Table_maximum_with_std.tex\", \"w\") as f:\n",
    "        f.write(latex_table)\n",
    "\n",
    "    print(f\"LaTeX table saved to {unique_output_dir}/Clustering_Metrics_Table_maximum_with_std.tex\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Load and preprocess the data\n",
    "X, y_encoded = load_and_preprocess_data(\"Data/synthetic_data.csv\")\n",
    "\n",
    "# Generate the results dataframe\n",
    "results_df = generate_dataframe(X, y_encoded, number_of_clusters=3, m=2.1, MAX_ITER=150, n_iter=100)\n",
    "\n",
    "# # Save intermediate DataFrame if needed\n",
    "# results_df.to_csv(\"clustering_metrics_summary.csv\", index=False)\n",
    "\n",
    "# Find and export best index values\n",
    "find_max_indices_for_n_pyth(results_df, unique_output_dir=\"results\", m=2.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100e57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
