{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to Data_Results\\run_2025-06-05_22-57-19/clustering_metrics_table.tex\n",
      "All outputs saved in: Data_Results\\run_2025-06-05_22-57-19\n",
      "LaTeX table saved to Data_Results\\run_2025-06-05_22-57-19/Clustering_Metrics_Table_maximum.tex\n"
     ]
    }
   ],
   "source": [
    "import My_final_clustering_program\n",
    "input = 'Data/iris.csv'\n",
    "My_final_clustering_program.main(input,3,'Data_Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from algorithms.new_euclidean_npy import nPyGK\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate anisotropic clusters with specific covariance structures\n",
    "def generate_elliptical_clusters():\n",
    "    n_samples = 500  # Total number of samples\n",
    "    \n",
    "    # Means of the clusters\n",
    "    means = [\n",
    "        [2, 8],\n",
    "        [8, 2],\n",
    "        [6, 6]\n",
    "    ]\n",
    "\n",
    "    # Covariance matrices for ellipsoidal clusters\n",
    "    covariances = [\n",
    "        [[3, 1], [1, 1]],  # Cluster 1: elongated along x-axis\n",
    "        [[1, -0.8], [-0.8, 2]],  # Cluster 2: tilted ellipse\n",
    "        [[2, 0], [0, 2]]  # Cluster 3: circular shape\n",
    "    ]\n",
    "\n",
    "    # Generate data for each cluster\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i, (mean, cov) in enumerate(zip(means, covariances)):\n",
    "        cluster_data = np.random.multivariate_normal(mean, cov, size=n_samples // len(means))\n",
    "        data.append(cluster_data)\n",
    "        labels.append(np.full(cluster_data.shape[0], i))\n",
    "\n",
    "    # Combine the data and labels\n",
    "    data = np.vstack(data)\n",
    "    labels = np.hstack(labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "data, labels = generate_elliptical_clusters()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fukuyama_sugeno_index(data, membership_matrix, centroids):\n",
    "    \"\"\"\n",
    "    Compute the Fukuyama-Sugeno Index (FS) for fuzzy clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: np.ndarray\n",
    "        The dataset, where each row is a data point and each column is a feature.\n",
    "    - membership_matrix: np.ndarray\n",
    "        Membership values, where each element u_ik represents the membership of point i to cluster k.\n",
    "        Shape: (n_samples, n_clusters)\n",
    "    - centroids: np.ndarray\n",
    "        Centroids of clusters. Shape: (n_clusters, n_features)\n",
    "        \n",
    "    Returns:\n",
    "    - FS: float\n",
    "        Fukuyama-Sugeno Index value.\n",
    "    \"\"\"\n",
    "    # Global centroid of the dataset\n",
    "    n_samples = data.shape[0]\n",
    "    global_centroid = np.mean(data, axis=0)\n",
    "\n",
    "    # Calculate the first term (compactness within clusters)\n",
    "    compactness = 0.0\n",
    "    for i in range(data.shape[0]):  # Iterate over all data points\n",
    "        for k in range(centroids.shape[0]):  # Iterate over all clusters\n",
    "            compactness += membership_matrix[i, k]**2 * np.linalg.norm(data[i] - centroids[k])**2\n",
    "\n",
    "    # Calculate the second term (separation between centroids and global centroid)\n",
    "    separation = 0.0\n",
    "    for k in range(centroids.shape[0]):  # Iterate over all clusters\n",
    "        separation += np.linalg.norm(centroids[k] - global_centroid)**2\n",
    "\n",
    "    # Compute the Fukuyama-Sugeno Index\n",
    "    FS = compactness/n_samples - separation\n",
    "    return FS\n",
    "\n",
    "# # Visualize the synthetic data\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=\"viridis\", edgecolor=\"k\", s=50)\n",
    "# plt.title(\"Synthetic Dataset with Elliptical Clusters\")\n",
    "# plt.xlabel(\"Feature 1\")\n",
    "# plt.ylabel(\"Feature 2\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Save the dataset to a file\n",
    "# np.savetxt(\"synthetic_data.csv\", data, delimiter=\",\", header=\"Feature1,Feature2\", comments=\"\")\n",
    "\n",
    "\n",
    "# kmeans_labels = kmeans.fit_predict(data)\n",
    "# kmeans_center = kmeans.cluster_centers_\n",
    "\n",
    "# Apply Fuzzy C-Means clustering\n",
    "\n",
    "# npygk_labels =  np.argmax(npygk_pred,axis=0) \n",
    "\n",
    "\n",
    "# print(f\"Silhouette Score for K-Means: {kmeans_score:.3f}\")\n",
    "# print(f\"Silhouette Score for Fuzzy C-Means: {npygk_score:.3f}\")\n",
    "\n",
    "# # Plot K-Means clustering results\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap=\"viridis\", s=30)\n",
    "# plt.title(\"K-Means Clustering\")\n",
    "# plt.xlabel(\"Feature 1\")\n",
    "# plt.ylabel(\"Feature 2\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot Fuzzy C-Means clustering results\n",
    "# plt.scatter(data[:, 0], data[:, 1], c=npygk_labels, cmap=\"viridis\", s=30)\n",
    "# plt.title(\"nPyGK Clustering\")\n",
    "# plt.xlabel(\"Feature 1\")\n",
    "# plt.ylabel(\"Feature 2\")\n",
    "# plt.show()\n",
    "c,l,i = k_means(data,n_clusters=3,random_state=42)\n",
    "k_score = silhouette_score(data,l)\n",
    "npygk = nPyGK(n_clusters=3,n_pyth=1,alpha=1)\n",
    "npygk_center =npygk.fit(data)\n",
    "npygk_labels = npygk.predict(data)\n",
    "# npygk_labels=np.argmax(npygk_labels,axis=1)\n",
    "fukuyama_sugeno_index(data,npygk_labels,npygk_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate anisotropic clusters with specific covariance structures\n",
    "def generate_elliptical_clusters():\n",
    "    n_samples = 500  # Total number of samples\n",
    "    \n",
    "    # Means of the clusters\n",
    "    means = [\n",
    "        [2, 8],\n",
    "        [8, 2],\n",
    "        [6, 6]\n",
    "    ]\n",
    "\n",
    "    # Covariance matrices for ellipsoidal clusters\n",
    "    covariances = [\n",
    "        [[3, 1], [1, 1]],  # Cluster 1: elongated along x-axis\n",
    "        [[1, -0.8], [-0.8, 2]],  # Cluster 2: tilted ellipse\n",
    "        [[2, 0], [0, 2]]  # Cluster 3: circular shape\n",
    "    ]\n",
    "\n",
    "    # Generate data for each cluster\n",
    "    data = []\n",
    "    for i, (mean, cov) in enumerate(zip(means, covariances)):\n",
    "        cluster_data = np.random.multivariate_normal(mean, cov, size=n_samples // len(means))\n",
    "        cluster_labels = np.full((cluster_data.shape[0], 1), i, dtype=int)  # Add labels as a column with integer values\n",
    "        cluster_data_with_labels = np.hstack((cluster_data, cluster_labels))  # Combine data and labels\n",
    "        data.append(cluster_data_with_labels)\n",
    "\n",
    "    # Combine all clusters into a single array\n",
    "    data = np.vstack(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "data = generate_elliptical_clusters()\n",
    "\n",
    "# Visualize the synthetic data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=data[:, 2], cmap=\"viridis\", edgecolor=\"k\", s=50)\n",
    "plt.title(\"Synthetic Dataset with Elliptical Clusters\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the dataset to a single .csv file\n",
    "np.savetxt(\"synthetic_data.csv\", data, delimiter=\",\", header=\"Feature1,Feature2,Label\", comments=\"\", fmt=\"%.6f,%.6f,%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "from algorithms.nPYGK import nPyGK\n",
    "\n",
    "def generate_clustering_graphs(df, unique_output_dir,m):\n",
    "\n",
    "    if not os.path.exists(unique_output_dir):\n",
    "        os.makedirs(unique_output_dir)\n",
    "\n",
    "    # Metrics to plot and their titles\n",
    "    metrics = [\n",
    "        \"partition coefficient\", \"fukuyama sugeno index\", \"xie beni index\",\n",
    "        \"silhouette score\", \"adjusted rand score\", \"adjusted mutual info score\",\n",
    "        \"homogeneity\", \"completeness\", \"v measure\"\n",
    "    ]\n",
    "    metric_titles = [\n",
    "        \"Partition Coefficient\", \"Fukuyama Sugeno Index\", \"Xie-Beni Index\",\n",
    "        \"Silhouette Score\", \"Adjusted Rand Score\", \"Adjusted Mutual Info Score\",\n",
    "        \"Homogeneity\", \"Completeness\", \"V-Measure\"\n",
    "    ]\n",
    "\n",
    "        # Get the specific row for n_pyth=1 and alpha=1\n",
    "    reference_row = df[(df[\"n_pyth\"] == 1) & (df[\"alpha\"] == 1)].iloc[0]\n",
    "    # Create a 3x3 grid for the plots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for n_pyth, group in df.groupby(\"n_pyth\"):\n",
    "            axes[i].plot(group[\"alpha\"], group[metric], marker=\"o\", label=f\"n_pyth={n_pyth}\")\n",
    "        # Add horizontal line for reference value\n",
    "        ref_value = reference_row[metric]\n",
    "        axes[i].axhline(y=ref_value, color='red', linestyle='--', label=f\"gk_value\")\n",
    "        axes[i].set_title(f\"{metric_titles[i]} vs Alpha\")\n",
    "        axes[i].set_xlabel(\"Alpha\")\n",
    "        axes[i].set_ylabel(metric_titles[i])\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    # Hide any unused subplots (in case the number of metrics is less than 9)\n",
    "    for j in range(len(metrics), 9):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{unique_output_dir}/clustering_metrics{m}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def load_and_preprocess_data(input_csv):\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    X = df.iloc[:, :-1].values  # Features\n",
    "    true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, true_labels_encoded\n",
    "\n",
    "def generate_dataframe(X,true_label_encoded, number_of_clusters, m, MAX_ITER):\n",
    "    data = []\n",
    "    # Loop through n_pyth and alpha values and get the results\n",
    "    for n_pyth in np.arange(1, 6):\n",
    "        for alpha in np.linspace(0.1, n_pyth, 10):\n",
    "            nPygk = nPyGK(n_clusters=number_of_clusters, m=m, max_iter=MAX_ITER, n_pyth=n_pyth, alpha=alpha)\n",
    "            cluster_centers = nPygk.fit(X)\n",
    "            predicted_labels = nPygk.predict(X)\n",
    "            metrices = calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded)\n",
    "            # Add n_pyth, alpha, and dictionary values (a, b, c, ..., g) to the data list\n",
    "            data.append({'n_pyth': n_pyth, 'alpha': alpha, **metrices})\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded):\n",
    "    # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(predicted_labels**2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "\n",
    "    cluster_dists = [\n",
    "        np.sum((predicted_labels[k,:]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "    # Separation \n",
    "    separation = np.min(\n",
    "        [\n",
    "            np.linalg.norm(cluster_centers[i] - cluster_centers[j])\n",
    "            for i in range(n_clusters)\n",
    "            for j in range(i + 1, n_clusters)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    se = compactness - separation\n",
    "    # xie beni index\n",
    "    xb = compactness /n_samples*separation if separation > 0 else np.inf\n",
    "    xb = np.minimum(xb,25)\n",
    "    \n",
    "\n",
    "    predicted_labels = np.argmax(predicted_labels,axis=0)\n",
    "    if len(np.unique(predicted_labels)) > 1:\n",
    "        ss = silhouette_score(X,predicted_labels)\n",
    "        ars = adjusted_rand_score(predicted_labels, true_label_encoded)\n",
    "        amis = adjusted_mutual_info_score(predicted_labels, true_label_encoded)\n",
    "        hh, cc, vv = homogeneity_completeness_v_measure(predicted_labels, true_label_encoded)\n",
    "    else:\n",
    "        ss = np.nan\n",
    "        ars = np.nan\n",
    "        amis = np.nan\n",
    "        hh, cc, vv = np.nan,np.nan,np.nan\n",
    "    return {\n",
    "        \"partition coefficient\": pc,\n",
    "        \"fukuyama sugeno index\": se,\n",
    "        \"xie beni index\": xb,\n",
    "        \"silhouette score\": ss,\n",
    "        \"adjusted rand score\": ars,\n",
    "        \"adjusted mutual info score\": amis,\n",
    "        \"homogeneity\": hh,\n",
    "        \"completeness\": cc,\n",
    "        \"v measure\": vv,\n",
    "    }\n",
    "\n",
    "def main(input_csv, number_of_clusters, output_dir):\n",
    "    X, true_labels_encoded = load_and_preprocess_data(input_csv)\n",
    "    MAX_ITER = 100\n",
    "    for m in np.arange(1.5,2.5,0.1):\n",
    "        df_pyth = generate_dataframe(X,true_labels_encoded, number_of_clusters, m, MAX_ITER)\n",
    "        generate_clustering_graphs(df_pyth, output_dir,m)\n",
    "\n",
    "main('synthetic_data.csv',3,'Data_Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    ")\n",
    "# Load dataset\n",
    "data = np.loadtxt(\"synthetic_data.csv\", delimiter=\",\", skiprows=1)  # Skip header row\n",
    "X = data[:, :2]  # Extract features\n",
    "true_value = data[:,2]\n",
    "# Apply K-Means Clustering\n",
    "k = 3  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "# # npygk = nPyGK(n_clusters=3,n_pyth=1,alpha=1,m=2.1)\n",
    "# # centroids =npygk.fit(data)\n",
    "# # labels = np.argmax(npygk.predict(data),axis=0)\n",
    "\n",
    "# # Function to calculate and plot ellipses\n",
    "# def plot_ellipse(mean, cov, ax, color):\n",
    "#     \"\"\" Draws an ellipse representing a Gaussian distribution with covariance matrix \"\"\"\n",
    "#     eigenvalues, eigenvectors = np.linalg.eigh(cov)  # Get eigenvalues & eigenvectors\n",
    "#     angle = np.degrees(np.arctan2(*eigenvectors[:, 1]))  # Compute ellipse orientation\n",
    "#     width, height = 2 * np.sqrt(eigenvalues)  # Scale by 2 standard deviations\n",
    "    \n",
    "#     # Create ellipse patch\n",
    "#     ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, \n",
    "#                       edgecolor=color, facecolor=\"none\", linewidth=2)\n",
    "#     ax.add_patch(ellipse)\n",
    "\n",
    "# # Compute and plot ellipses\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# ax.scatter(X[:, 0], X[:, 1], c=labels, cmap=\"viridis\", edgecolor=\"k\", s=50)\n",
    "# ax.scatter(centroids[:, 0], centroids[:, 1], c=\"red\", marker=\"x\", s=200, label=\"Centroids\")\n",
    "\n",
    "# # Calculate covariance for each cluster and draw ellipses\n",
    "# for i in range(k):\n",
    "#     cluster_points = X[labels == i]  # Get points of the cluster\n",
    "#     cov_matrix = np.cov(cluster_points, rowvar=False)  # Compute covariance matrix\n",
    "#     plot_ellipse(centroids[i], cov_matrix, ax, color=\"black\")\n",
    "\n",
    "# plt.title(\"K-Means Clustering with Elliptical Representation\")\n",
    "# plt.xlabel(\"Feature 1\")\n",
    "# plt.ylabel(\"Feature 2\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.axis(\"equal\")  # Ensure proper aspect ratio\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Wine.csv')\n",
    "# Define metric columns\n",
    "metric_columns = [col for col in df.columns if col not in [\"n_pyth\", \"alpha\"]]\n",
    "\n",
    "# Get reference row for n_pyth = 1 and alpha = 1\n",
    "reference_row = df[(df[\"n_pyth\"] == 1) & (df[\"alpha\"] == 1)].iloc[0]\n",
    "\n",
    "# Filter the dataset for n_pyth = 1\n",
    "filtered_df = df[df[\"n_pyth\"] == 5]\n",
    "\n",
    "# Clustering indices categories\n",
    "less_than_indices = [\"xie beni index\", \"fukuyama sugeno index\"]\n",
    "greater_than_indices = [col for col in metric_columns if col not in less_than_indices]\n",
    "\n",
    "# Store results\n",
    "alpha_lists = {}\n",
    "\n",
    "for col in metric_columns:\n",
    "    if col in less_than_indices:\n",
    "        # Find alpha values where score is LESS than reference\n",
    "        alpha_filtered = filtered_df[filtered_df[col] < reference_row[col]][\"alpha\"].tolist()\n",
    "    else:\n",
    "        # Find alpha values where score is GREATER than reference\n",
    "        alpha_filtered = filtered_df[filtered_df[col] > reference_row[col]][\"alpha\"].tolist()\n",
    "    \n",
    "    # Store the list\n",
    "    alpha_lists[col] = alpha_filtered\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "alpha_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in alpha_lists.items()]))\n",
    "df_new = alpha_df.drop('Unnamed: 0',axis=1)\n",
    "df_new.to_csv(\"Wine_better.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "a = np.array([32, 34, 37, 39, 42, 44, 46, 48, 49, 50, 52, 53, 55, 57, 58, 60, 61, 63, 64, 65, 68, 69, 70, 71, 73, 76, 80])\n",
    "\n",
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(a,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25*24/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from algorithms.gk import GK\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score,adjusted_rand_score,adjusted_mutual_info_score\n",
    "\n",
    "df = pd.read_csv(\"Data/wine.csv\")\n",
    "X = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='random',n_init = 1,max_iter=100,random_state=42)\n",
    "predicted_labels_kmean = kmeans.fit_predict(X)\n",
    "\n",
    "# nPygk = GK(n_clusters=3, m=2, max_iter=300)\n",
    "\n",
    "# cluster_centers = nPygk.fit(X)\n",
    "# predicted_labels = nPygk.predict(X)\n",
    "# predicted_labels\n",
    "s_kmean = silhouette_score(X,predicted_labels_kmean)\n",
    "ars_kmean = adjusted_rand_score(predicted_labels_kmean, true_labels_encoded)\n",
    "ami_kmean = adjusted_mutual_info_score(predicted_labels_kmean,true_labels_encoded)\n",
    "s_kmean,ars_kmean,ami_kmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_centroids(data, n_clusters, random_state=None):\n",
    "    \"\"\"\n",
    "    Selects n_clusters observations at random from the data as initial centroids.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray or list): The dataset (rows = observations, columns = features).\n",
    "    - n_clusters (int): Number of clusters (initial centroids).\n",
    "    - random_state (int, optional): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Randomly selected initial centroids.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)  # For reproducibility\n",
    "\n",
    "    random_indices = np.random.choice(data.shape[0], n_clusters, replace=False)\n",
    "    return data[random_indices]\n",
    "\n",
    "# Example usage\n",
    "data = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0],\n",
    "    [7.0, 8.0],\n",
    "    [9.0, 10.0]\n",
    "])\n",
    "\n",
    "n_clusters = 2\n",
    "initial_centroids = initialize_centroids(data, n_clusters, random_state=42)\n",
    "print(\"Initial Centroids:\\n\", initial_centroids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from algorithms.fcm import FCM\n",
    "\n",
    "from algorithms.nPYGK import nPyGK\n",
    "# from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "\n",
    "def calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded):\n",
    "    # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(predicted_labels**2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "\n",
    "    cluster_dists = [\n",
    "        np.sum((predicted_labels[k,:]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "    # Separation \n",
    "    separation = np.min(\n",
    "        [\n",
    "            np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "            for i in range(n_clusters)\n",
    "            for j in range(i + 1, n_clusters)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    se = compactness - separation\n",
    "    # xie beni index\n",
    "    xb = compactness /n_samples*separation if separation > 0 else np.inf\n",
    "    # xb = np.minimum(xb,25)\n",
    "    \n",
    "\n",
    "    predicted_labels = np.argmax(predicted_labels,axis=0)\n",
    "    if len(np.unique(predicted_labels)) > 1:\n",
    "        ss = silhouette_score(X,predicted_labels)\n",
    "        ars = adjusted_rand_score(predicted_labels, true_label_encoded)\n",
    "        amis = adjusted_mutual_info_score(predicted_labels, true_label_encoded)\n",
    "        hh, cc, vv = homogeneity_completeness_v_measure(predicted_labels, true_label_encoded)\n",
    "    return {\n",
    "        \"partition coefficient\": pc,\n",
    "        \"fukuyama sugeno index\": se,\n",
    "        \"xie beni index\": xb,\n",
    "        \"silhouette score\": ss,\n",
    "        \"adjusted rand score\": ars,\n",
    "        \"adjusted mutual info score\": amis,\n",
    "        \"homogeneity\": hh,\n",
    "        \"completeness\": cc,\n",
    "        \"v measure\": vv,\n",
    "    }\n",
    "\n",
    "df = pd.read_csv(\"Data/column_2C.csv\")\n",
    "df[\"class\"] = df[\"class\"].map({'Normal': 0, 'Abnormal': 1})\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_data_encoded = scaler.fit_transform(X_data)\n",
    "\n",
    "X_data_encoded\n",
    "nPyGk = nPyGK(n_clusters=2,max_iter=300,n_pyth=1,alpha=1)\n",
    "gk_centers = nPyGk.fit(X_data_encoded)\n",
    "gk_labels = nPyGk.predict(X_data_encoded)\n",
    "\n",
    "calculate_indices(X_data_encoded, gk_labels, gk_centers,true_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For n-Pythagorean FCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'partition coefficient': 0.6342968339848633,\n",
       " 'fukuyama sugeno index': -163.5849629712171,\n",
       " 'xie beni index': 0.8876399733064123,\n",
       " 'silhouette score': 0.3224385414009555,\n",
       " 'adjusted rand score': 0.7436826319432358,\n",
       " 'adjusted mutual info score': 0.763083127524581,\n",
       " 'homogeneity': 0.7717917344958115,\n",
       " 'completeness': 0.7603645798041669,\n",
       " 'v measure': 0.7660355440487253}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GK\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from algorithms.gk import GK\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded):\n",
    "    # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(predicted_labels**2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "    # Compactness (within-cluster dispersion)\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    cluster_dists = [\n",
    "        np.sum((predicted_labels[k, :]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    # Minimum separation distance (between-cluster dispersion)\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Xie-Beni Index\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(predicted_labels[k, :]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Fukuyama-Sugeno Index\n",
    "    se = compactness - separation_fs\n",
    "    \n",
    "\n",
    "    predicted_labels = np.argmax(predicted_labels,axis=0)\n",
    "    if len(np.unique(predicted_labels)) > 1:\n",
    "        ss = silhouette_score(X,predicted_labels)\n",
    "        ars = adjusted_rand_score(predicted_labels, true_label_encoded)\n",
    "        amis = adjusted_mutual_info_score(predicted_labels, true_label_encoded)\n",
    "        hh, cc, vv = homogeneity_completeness_v_measure(predicted_labels, true_label_encoded)\n",
    "    return {\n",
    "        \"partition coefficient\": pc,\n",
    "        \"fukuyama sugeno index\": se,\n",
    "        \"xie beni index\": xb,\n",
    "        \"silhouette score\": ss,\n",
    "        \"adjusted rand score\": ars,\n",
    "        \"adjusted mutual info score\": amis,\n",
    "        \"homogeneity\": hh,\n",
    "        \"completeness\": cc,\n",
    "        \"v measure\": vv,\n",
    "    }\n",
    "\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "# df[\"class\"] = df[\"class\"].map({'Normal': 0, 'Abnormal': 1})\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_data_encoded = scaler.fit_transform(X_data)\n",
    "\n",
    "nPyGk = GK(n_clusters=3,m=2.1,max_iter=100)\n",
    "gk_centers = nPyGk.fit(X_data_encoded)\n",
    "gk_labels = nPyGk.predict(X_data_encoded)\n",
    "\n",
    "calculate_indices(X_data_encoded, gk_labels, gk_centers,true_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from algorithms.fcm import FCM\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "\n",
    "def calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded):\n",
    "    # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(predicted_labels**2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "\n",
    "    cluster_dists = [\n",
    "        np.sum((predicted_labels[:,k]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    # Minimum separation distance (between-cluster dispersion)\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Xie-Beni Index\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(predicted_labels[:,k]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Fukuyama-Sugeno Index\n",
    "    se = compactness - separation_fs\n",
    "    \n",
    "\n",
    "    predicted_labels = np.argmax(predicted_labels,axis=-1)\n",
    "    if len(np.unique(predicted_labels)) > 1:\n",
    "        ss = silhouette_score(X,predicted_labels)\n",
    "        ars = adjusted_rand_score(predicted_labels, true_label_encoded)\n",
    "        amis = adjusted_mutual_info_score(predicted_labels, true_label_encoded)\n",
    "        hh, cc, vv = homogeneity_completeness_v_measure(predicted_labels, true_label_encoded)\n",
    "    return {\n",
    "        \"partition coefficient\": pc,\n",
    "        \"fukuyama sugeno index\": se,\n",
    "        \"xie beni index\": xb,\n",
    "        \"silhouette score\": ss,\n",
    "        \"adjusted rand score\": ars,\n",
    "        \"adjusted mutual info score\": amis,\n",
    "        \"homogeneity\": hh,\n",
    "        \"completeness\": cc,\n",
    "        \"v measure\": vv,\n",
    "    }\n",
    "    \n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "# df[\"class\"] = df[\"class\"].map({'Normal': 0, 'Abnormal': 1})\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_data_encoded = scaler.fit_transform(X_data)\n",
    "\n",
    "nPyGk = FCM(n_clusters=3,m=2,max_iter=300)\n",
    "gk_centers = nPyGk.fit(X_data_encoded)\n",
    "gk_labels = nPyGk.predict(X_data_encoded)\n",
    "\n",
    "calculate_indices(X_data_encoded, gk_labels, gk_centers,true_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "\n",
    "def calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded):\n",
    "    # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(predicted_labels**2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "\n",
    "    cluster_dists = [\n",
    "        np.sum((predicted_labels[k]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    # Minimum separation distance (between-cluster dispersion)\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Xie-Beni Index\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(predicted_labels[k]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Fukuyama-Sugeno Index\n",
    "    se = compactness - separation_fs\n",
    "    \n",
    "\n",
    "    # predicted_labels = np.argmax(predicted_labels,axis=0)\n",
    "    if len(np.unique(predicted_labels)) > 1:\n",
    "        ss = silhouette_score(X,predicted_labels)\n",
    "        ars = adjusted_rand_score(predicted_labels, true_label_encoded)\n",
    "        amis = adjusted_mutual_info_score(predicted_labels, true_label_encoded)\n",
    "        hh, cc, vv = homogeneity_completeness_v_measure(predicted_labels, true_label_encoded)\n",
    "    return {\n",
    "        \"partition coefficient\": pc,\n",
    "        \"fukuyama sugeno index\": se,\n",
    "        \"xie beni index\": xb,\n",
    "        \"silhouette score\": ss,\n",
    "        \"adjusted rand score\": ars,\n",
    "        \"adjusted mutual info score\": amis,\n",
    "        \"homogeneity\": hh,\n",
    "        \"completeness\": cc,\n",
    "        \"v measure\": vv,\n",
    "    }\n",
    "\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "# df[\"class\"] = df[\"class\"].map({'Normal': 0, 'Abnormal': 1})\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_data_encoded = scaler.fit_transform(X_data)\n",
    "\n",
    "kmean = KMeans(n_clusters=3,init='random',n_init=1,max_iter=300,random_state=51).fit(X_data_encoded)\n",
    "gk_centers = kmean.cluster_centers_\n",
    "gk_labels = kmean.labels_\n",
    "\n",
    "\n",
    "calculate_indices(X_data_encoded, gk_labels, gk_centers,true_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import My_final_clustering_program\n",
    "input = 'Data/iris.csv'\n",
    "My_final_clustering_program.main(input,3,'Data_Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded):\n",
    "    # Compactness (within-cluster dispersion)\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    cluster_dists = [\n",
    "        np.sum((predicted_labels[k, :]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    # Minimum separation distance (between-cluster dispersion)\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Xie-Beni Index\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(predicted_labels[k, :]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Fukuyama-Sugeno Index\n",
    "    fsi = compactness - separation_fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from algorithms.nPyGK import nPyGK\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_indices(X, predicted_labels, cluster_centers,true_label_encoded):\n",
    "    # Number of samples and clusters\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(predicted_labels**2) / n_samples\n",
    "    # Separation Index (SE)\n",
    "    # Compactness (within-cluster dispersion)\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(cluster_centers)\n",
    "    cluster_dists = [\n",
    "        np.sum((predicted_labels[k, :]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    # Minimum separation distance (between-cluster dispersion)\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Xie-Beni Index\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for Fukuyama-Sugeno Index\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(predicted_labels[k, :]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Fukuyama-Sugeno Index\n",
    "    se = compactness - separation_fs\n",
    "    \n",
    "\n",
    "    predicted_labels = np.argmax(predicted_labels,axis=0)\n",
    "    if len(np.unique(predicted_labels)) > 1:\n",
    "        ss = silhouette_score(X,predicted_labels)\n",
    "        ars = adjusted_rand_score(predicted_labels, true_label_encoded)\n",
    "        amis = adjusted_mutual_info_score(predicted_labels, true_label_encoded)\n",
    "        hh, cc, vv = homogeneity_completeness_v_measure(predicted_labels, true_label_encoded)\n",
    "    return {\n",
    "        \"partition coefficient\": pc,\n",
    "        \"fukuyama sugeno index\": se,\n",
    "        \"xie beni index\": xb,\n",
    "        \"silhouette score\": ss,\n",
    "        \"adjusted rand score\": ars,\n",
    "        \"adjusted mutual info score\": amis,\n",
    "        \"homogeneity\": hh,\n",
    "        \"completeness\": cc,\n",
    "        \"v measure\": vv,\n",
    "    }\n",
    "\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "# df[\"class\"] = df[\"class\"].map({'Normal': 0, 'Abnormal': 1})\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_data_encoded = scaler.fit_transform(X_data)\n",
    "\n",
    "nPyGk = nPyGK(n_clusters=3,m=2.1,max_iter=100,n_pyth=1,alpha=1)\n",
    "gk_centers = nPyGk.fit(X_data_encoded)\n",
    "gk_labels = nPyGk.predict(X_data_encoded)\n",
    "\n",
    "calculate_indices(X_data_encoded, gk_labels, gk_centers,true_labels_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Graph is not fully connected\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not find the number of physical cores\")\n",
    "\n",
    "# Limit number of threads to prevent memory leak in KMeans\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs  # or your own dataset\n",
    "\n",
    "# Step 1: Create or load data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Step 2: Apply Spectral Clustering\n",
    "sc = SpectralClustering(n_clusters=4, affinity='nearest_neighbors',n_neighbors=10, random_state=42)\n",
    "labels = sc.fit_predict(X)\n",
    "\n",
    "# Step 3: Compute Silhouette Score\n",
    "score = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Score: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Step 1: Create data or use your own\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "\n",
    "# Step 2: Apply Hierarchical Clustering\n",
    "hc = AgglomerativeClustering(n_clusters=4, linkage='ward')  # or 'complete', 'average', 'single'\n",
    "labels = hc.fit_predict(X)\n",
    "\n",
    "# Step 3: Compute Silhouette Score\n",
    "score = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'partition coefficient': 0.6120689655172413, 'silhouette score': 0.6558885287002018, 'adjusted rand score': 0.6309344087637648, 'adjusted mutual info score': 0.7577941598533724, 'homogeneity': 1.0000000000000002, 'completeness': 0.6131861417870639, 'v measure': 0.7602174676603474}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score,\n",
    "\n",
    ")\n",
    "\n",
    "def calculate_indices(X, predicted_labels,true_label_encoded):\n",
    "    n_samples = X.shape[0]\n",
    "    pc = np.sum(predicted_labels**2) / n_samples\n",
    "    if len(np.unique(predicted_labels)) > 1:\n",
    "        ss = silhouette_score(X,predicted_labels)\n",
    "        ars = adjusted_rand_score(predicted_labels, true_label_encoded)\n",
    "        amis = adjusted_mutual_info_score(predicted_labels, true_label_encoded)\n",
    "        hh, cc, vv = homogeneity_completeness_v_measure(predicted_labels, true_label_encoded)\n",
    "    return {\n",
    "        \"partition coefficient\": pc,\n",
    "        # \"fukuyama sugeno index\": se,\n",
    "        # \"xie beni index\": xb,\n",
    "        \"silhouette score\": ss,\n",
    "        \"adjusted rand score\": ars,\n",
    "        \"adjusted mutual info score\": amis,\n",
    "        \"homogeneity\": hh,\n",
    "        \"completeness\": cc,\n",
    "        \"v measure\": vv,\n",
    "    }\n",
    "\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "X_data = df.iloc[:, :-1].values  # Features\n",
    "true_labels = df.iloc[:, -1].values  # True labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Apply DBSCAN\n",
    "db = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "\n",
    "# Step 3: Filter out noise points (label = -1)\n",
    "# Step 3: Filter out noise points (label = -1)\n",
    "mask = labels != -1\n",
    "X_data_encoded = X[mask]\n",
    "predicted_labels_db = labels[mask]\n",
    "true_labels_encoded_filtered = true_labels_encoded[mask]\n",
    "\n",
    "# Step 4: Compute cluster validity indices\n",
    "results = calculate_indices(X_data_encoded, predicted_labels_db, true_labels_encoded_filtered)\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, adjusted_mutual_info_score, homogeneity_completeness_v_measure\n",
    "import numpy as np\n",
    "\n",
    "def calculate_indices_hard(X, labels, true_label_encoded):\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Compute cluster centers\n",
    "    cluster_centers = np.array([\n",
    "        np.mean(X[labels == label], axis=0)\n",
    "        for label in unique_labels\n",
    "    ])\n",
    "\n",
    "    # Create binary membership matrix (hard clustering)\n",
    "    membership = np.zeros((n_clusters, n_samples))\n",
    "    for i, label in enumerate(labels):\n",
    "        cluster_idx = np.where(unique_labels == label)[0][0]\n",
    "        membership[cluster_idx, i] = 1\n",
    "\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(membership**2) / n_samples\n",
    "\n",
    "    # Compactness (within-cluster dispersion)\n",
    "    cluster_dists = [\n",
    "        np.sum((membership[k]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    # Minimum separation distance (between-cluster dispersion)\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ])\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    # Global centroid for Fukuyama-Sugeno Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation term for FS Index\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(membership[k]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "    se = compactness - separation_fs\n",
    "\n",
    "    # External validation\n",
    "    ss = silhouette_score(X, labels)\n",
    "    ars = adjusted_rand_score(labels, true_label_encoded)\n",
    "    amis = adjusted_mutual_info_score(labels, true_label_encoded)\n",
    "    hh, cc, vv = homogeneity_completeness_v_measure(labels, true_label_encoded)\n",
    "\n",
    "    return {\n",
    "        \"partition coefficient\": round(pc, 4),\n",
    "        \"fukuyama sugeno index\": round(se,4),\n",
    "        \"xie beni index\": round(xb,4),\n",
    "        \"silhouette score\": round(ss,4),\n",
    "        \"adjusted rand score\": round(ars,4),\n",
    "        \"adjusted mutual info score\": round(amis,4),\n",
    "        \"homogeneity\": round(hh,4),\n",
    "        \"completeness\": round(cc,4),\n",
    "        \"v measure\": round(vv,4),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'partition coefficient': 1.0, 'fukuyama sugeno index': -430.728, 'xie beni index': 0.4983, 'silhouette score': 0.4652, 'adjusted rand score': 0.213, 'adjusted mutual info score': 0.4563, 'homogeneity': 0.3313, 'completeness': 0.9039, 'v measure': 0.4848}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"Data/synthetic_data.csv\")\n",
    "X_data = df.iloc[:, :-1].values\n",
    "true_labels = df.iloc[:, -1].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "\n",
    "# Apply DBSCAN\n",
    "db = DBSCAN(eps=0.1, min_samples=5)\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "# Filter noise\n",
    "mask = labels != -1\n",
    "X_filtered = X[mask]\n",
    "labels_filtered = labels[mask]\n",
    "true_filtered = true_labels_encoded[mask]\n",
    "\n",
    "# Compute indices\n",
    "results = calculate_indices_hard(X_filtered, labels_filtered, true_filtered)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Clustering Results:\n",
      "partition coefficient: 1.0\n",
      "fukuyama sugeno index: -302.2475\n",
      "xie beni index: 0.2621\n",
      "silhouette score: 0.4467\n",
      "adjusted rand score: 0.6153\n",
      "adjusted mutual info score: 0.6713\n",
      "homogeneity: 0.694\n",
      "completeness: 0.6579\n",
      "v measure: 0.6755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"Data/iris.csv\")\n",
    "X_data = df.iloc[:, :-1].values\n",
    "true_labels = df.iloc[:, -1].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "hc = AgglomerativeClustering(n_clusters=3)\n",
    "labels = hc.fit_predict(X)\n",
    "\n",
    "results = calculate_indices_hard(X, labels, true_labels_encoded)\n",
    "print(\"Hierarchical Clustering Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral Clustering Results:\n",
      "partition coefficient: 1.0\n",
      "fukuyama sugeno index: -567.0962\n",
      "xie beni index: 0.1435\n",
      "silhouette score: 0.4631\n",
      "adjusted rand score: 0.6451\n",
      "adjusted mutual info score: 0.6743\n",
      "homogeneity: 0.686\n",
      "completeness: 0.6654\n",
      "v measure: 0.6755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "df = pd.read_csv(\"Data/synthetic_data.csv\")\n",
    "X_data = df.iloc[:, :-1].values\n",
    "true_labels = df.iloc[:, -1].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_data)\n",
    "\n",
    "sc = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n",
    "labels = sc.fit_predict(X)\n",
    "\n",
    "results = calculate_indices_hard(X, labels, true_labels_encoded)\n",
    "print(\"Spectral Clustering Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from fcmeans import FCM\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def bootstrap_clustering(X, y_true, n_clusters=3, n_bootstrap=100, algorithm='kmeans'):\n",
    "    scores = {\n",
    "        'silhouette': [],\n",
    "        'adjusted_rand': []\n",
    "    }\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        # Bootstrap resample\n",
    "        indices = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[indices]\n",
    "        y_sample = y_true[indices]\n",
    "\n",
    "        if algorithm == 'kmeans':\n",
    "            model = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "            labels = model.fit_predict(X_sample)\n",
    "        elif algorithm == 'fcm':\n",
    "            model = FCM(n_clusters=n_clusters, random_state=42)\n",
    "            model.fit(X_sample)\n",
    "            labels = model.u.argmax(axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported algorithm\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        sil_score = silhouette_score(X_sample, labels)\n",
    "        ari_score = adjusted_rand_score(y_sample, labels)\n",
    "\n",
    "        scores['silhouette'].append(sil_score)\n",
    "        scores['adjusted_rand'].append(ari_score)\n",
    "\n",
    "    # Aggregate statistics\n",
    "    results = {\n",
    "        'silhouette_mean': np.mean(scores['silhouette']),\n",
    "        'silhouette_std': np.std(scores['silhouette']),\n",
    "        'adjusted_rand_mean': np.mean(scores['adjusted_rand']),\n",
    "        'adjusted_rand_std': np.std(scores['adjusted_rand'])\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans: {'silhouette_mean': 0.47705661677776606, 'silhouette_std': 0.02249815298374304, 'adjusted_rand_mean': 0.610658891699671, 'adjusted_rand_std': 0.06835030386472617}\n",
      "FCM: {'silhouette_mean': 0.47141260328122697, 'silhouette_std': 0.02319973024535792, 'adjusted_rand_mean': 0.6339739670185445, 'adjusted_rand_std': 0.06820960594167967}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# KMeans bootstrap\n",
    "kmeans_stats = bootstrap_clustering(X, y, n_clusters=3, algorithm='kmeans')\n",
    "print(\"KMeans:\", kmeans_stats)\n",
    "\n",
    "# FCM bootstrap\n",
    "fcm_stats = bootstrap_clustering(X, y, n_clusters=3, algorithm='fcm')\n",
    "print(\"FCM:\", fcm_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from fcmeans import FCM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, adjusted_mutual_info_score, homogeneity_completeness_v_measure\n",
    "\n",
    "def calculate_indices_hard(X, labels, true_label_encoded):\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Compute cluster centers\n",
    "    cluster_centers = np.array([\n",
    "        np.mean(X[labels == label], axis=0)\n",
    "        for label in unique_labels\n",
    "    ])\n",
    "\n",
    "    # Binary membership matrix\n",
    "    membership = np.zeros((n_clusters, n_samples))\n",
    "    for i, label in enumerate(labels):\n",
    "        cluster_idx = np.where(unique_labels == label)[0][0]\n",
    "        membership[cluster_idx, i] = 1\n",
    "\n",
    "    # Partition Coefficient\n",
    "    pc = np.sum(membership**2) / n_samples\n",
    "\n",
    "    # Compactness\n",
    "    cluster_dists = [\n",
    "        np.sum((membership[k]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    # Separation\n",
    "    if n_clusters > 1:\n",
    "        separation = np.min([\n",
    "            np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "            for i in range(n_clusters)\n",
    "            for j in range(i + 1, n_clusters)\n",
    "        ])\n",
    "    else:\n",
    "        separation = 0\n",
    "\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    # Global centroid\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Fukuyama-Sugeno Index\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(membership[k]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "    fsi = compactness - separation_fs\n",
    "\n",
    "    # External indices\n",
    "    ss = silhouette_score(X, labels)\n",
    "    ars = adjusted_rand_score(true_label_encoded, labels)\n",
    "    amis = adjusted_mutual_info_score(true_label_encoded, labels)\n",
    "    hh, cc, vv = homogeneity_completeness_v_measure(true_label_encoded, labels)\n",
    "\n",
    "    return {\n",
    "        \"PC\": pc,\n",
    "        \"FSI\": fsi,\n",
    "        \"XBI\": xb,\n",
    "        \"Silhouette\": ss,\n",
    "        \"ARS\": ars,\n",
    "        \"AMI\": amis,\n",
    "        \"Homogeneity\": hh,\n",
    "        \"Completeness\": cc,\n",
    "        \"V-measure\": vv\n",
    "    }\n",
    "\n",
    "def bootstrap_clustering(X, y_true, n_clusters=3, n_bootstrap=100, algorithm='kmeans'):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    all_metrics = {\n",
    "        \"PC\": [], \"FSI\": [], \"XBI\": [], \"Silhouette\": [],\n",
    "        \"ARS\": [], \"AMI\": [], \"Homogeneity\": [], \"Completeness\": [], \"V-measure\": []\n",
    "    }\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        # Bootstrap resample\n",
    "        indices = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[indices]\n",
    "        y_sample = y_true[indices]\n",
    "\n",
    "        # Apply clustering\n",
    "        if algorithm == 'kmeans':\n",
    "            model = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "            labels = model.fit_predict(X_sample)\n",
    "        elif algorithm == 'fcm':\n",
    "            model = FCM(n_clusters=n_clusters, random_state=42)\n",
    "            model.fit(X_sample)\n",
    "            labels = model.u.argmax(axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported algorithm\")\n",
    "\n",
    "        # Compute clustering indices\n",
    "        metrics = calculate_indices_hard(X_sample, labels, y_sample)\n",
    "\n",
    "        # Collect metrics\n",
    "        for key in all_metrics:\n",
    "            all_metrics[key].append(metrics[key])\n",
    "\n",
    "    # Compute mean and std\n",
    "    results = {\n",
    "        key: {\n",
    "            \"mean\": round(np.mean(values), 4),\n",
    "            \"std\": round(np.std(values), 4)\n",
    "        }\n",
    "        for key, values in all_metrics.items()\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans Bootstrap Results:\n",
      "PC: Mean = 1.0, Std = 0.0\n",
      "FSI: Mean = -313.2961, Std = 43.0911\n",
      "XBI: Mean = 0.2898, Std = 0.1007\n",
      "Silhouette: Mean = 0.479, Std = 0.0233\n",
      "ARS: Mean = 0.6086, Std = 0.0837\n",
      "AMI: Mean = 0.6582, Std = 0.0455\n",
      "Homogeneity: Mean = 0.6529, Std = 0.0519\n",
      "Completeness: Mean = 0.6745, Std = 0.0504\n",
      "V-measure: Mean = 0.6626, Std = 0.0449\n",
      "\n",
      "FCM Bootstrap Results:\n",
      "PC: Mean = 1.0, Std = 0.0\n",
      "FSI: Mean = -323.9952, Std = 31.2918\n",
      "XBI: Mean = 0.2719, Std = 0.044\n",
      "Silhouette: Mean = 0.4727, Std = 0.0243\n",
      "ARS: Mean = 0.6359, Std = 0.0693\n",
      "AMI: Mean = 0.6671, Std = 0.0447\n",
      "Homogeneity: Mean = 0.6702, Std = 0.0449\n",
      "Completeness: Mean = 0.6725, Std = 0.044\n",
      "V-measure: Mean = 0.6713, Std = 0.0441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# KMeans\n",
    "kmeans_bootstrap = bootstrap_clustering(X, y, n_clusters=3, algorithm='kmeans', n_bootstrap=100)\n",
    "print(\"KMeans Bootstrap Results:\")\n",
    "for metric, stat in kmeans_bootstrap.items():\n",
    "    print(f\"{metric}: Mean = {stat['mean']}, Std = {stat['std']}\")\n",
    "\n",
    "# FCM\n",
    "fcm_bootstrap = bootstrap_clustering(X, y, n_clusters=3, algorithm='fcm', n_bootstrap=100)\n",
    "print(\"\\nFCM Bootstrap Results:\")\n",
    "for metric, stat in fcm_bootstrap.items():\n",
    "    print(f\"{metric}: Mean = {stat['mean']}, Std = {stat['std']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from fcmeans import FCM\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Your clustering evaluation function for hard labels\n",
    "def calculate_indices_hard(X, labels, true_label_encoded):\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    cluster_centers = np.array([\n",
    "        np.mean(X[labels == label], axis=0)\n",
    "        for label in unique_labels\n",
    "    ])\n",
    "\n",
    "    membership = np.zeros((n_clusters, n_samples))\n",
    "    for i, label in enumerate(labels):\n",
    "        cluster_idx = np.where(unique_labels == label)[0][0]\n",
    "        membership[cluster_idx, i] = 1\n",
    "\n",
    "    pc = np.sum(membership**2) / n_samples\n",
    "\n",
    "    cluster_dists = [\n",
    "        np.sum((membership[k]**2) * np.linalg.norm(X - cluster_centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    compactness = np.sum(cluster_dists)\n",
    "\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(cluster_centers[i] - cluster_centers[j])**2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ]) if n_clusters > 1 else np.inf\n",
    "\n",
    "    xb = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "    separation_fs = np.sum([\n",
    "        np.sum(membership[k]**2) * np.linalg.norm(cluster_centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "    se = compactness - separation_fs\n",
    "\n",
    "    ss = silhouette_score(X, labels)\n",
    "    ars = adjusted_rand_score(true_label_encoded, labels)\n",
    "    amis = adjusted_mutual_info_score(true_label_encoded, labels)\n",
    "    hh, cc, vv = homogeneity_completeness_v_measure(true_label_encoded, labels)\n",
    "\n",
    "    return {\n",
    "        \"PC\": pc,\n",
    "        \"FSI\": se,\n",
    "        \"XBI\": xb,\n",
    "        \"Silhouette\": ss,\n",
    "        \"ARS\": ars,\n",
    "        \"AMI\": amis,\n",
    "        \"Homogeneity\": hh,\n",
    "        \"Completeness\": cc,\n",
    "        \"V-measure\": vv,\n",
    "    }\n",
    "\n",
    "# Bootstrap function\n",
    "def bootstrap_evaluation(X, true_labels, n_clusters=3, n_iterations=100, eps=0.5, min_samples=5):\n",
    "    results = {\n",
    "        \"KMeans\": [],\n",
    "        \"FCM\": [],\n",
    "        \"DBSCAN\": [],\n",
    "        \"Spectral\": [],\n",
    "        \"Hierarchical\": []\n",
    "        # \"GK\": []  add if implemented\n",
    "    }\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    true_encoded = label_encoder.fit_transform(true_labels)\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        X_sample, y_sample = resample(X, true_encoded)\n",
    "\n",
    "        # KMeans\n",
    "        kmeans_labels = KMeans(n_clusters=n_clusters, n_init=10).fit_predict(X_sample)\n",
    "        results[\"KMeans\"].append(calculate_indices_hard(X_sample, kmeans_labels, y_sample))\n",
    "\n",
    "        # FCM\n",
    "        fcm = FCM(n_clusters=n_clusters)\n",
    "        fcm.fit(X_sample)\n",
    "        fcm_labels = fcm.predict(X_sample)\n",
    "        results[\"FCM\"].append(calculate_indices_hard(X_sample, fcm_labels, y_sample))\n",
    "\n",
    "        # DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        db_labels = dbscan.fit_predict(X_sample)\n",
    "        if len(set(db_labels)) > 1 and -1 in db_labels:\n",
    "            mask = db_labels != -1\n",
    "            results[\"DBSCAN\"].append(calculate_indices_hard(X_sample[mask], db_labels[mask], y_sample[mask]))\n",
    "\n",
    "        # Spectral\n",
    "        spectral = SpectralClustering(n_clusters=n_clusters, assign_labels='kmeans', affinity='rbf')\n",
    "        spectral_labels = spectral.fit_predict(X_sample)\n",
    "        results[\"Spectral\"].append(calculate_indices_hard(X_sample, spectral_labels, y_sample))\n",
    "\n",
    "        # Hierarchical\n",
    "        hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        hc_labels = hc.fit_predict(X_sample)\n",
    "        results[\"Hierarchical\"].append(calculate_indices_hard(X_sample, hc_labels, y_sample))\n",
    "\n",
    "    # Mean and Std Calculation\n",
    "    summary = {}\n",
    "    for method, scores in results.items():\n",
    "        if scores:\n",
    "            df = pd.DataFrame(scores)\n",
    "            summary[method] = {\n",
    "                'mean': df.mean().round(4).to_dict(),\n",
    "                'std': df.std().round(4).to_dict()\n",
    "            }\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\manas\\anaconda3\\envs\\MyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (150,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    128\u001b[39m iris = load_iris()\n\u001b[32m    129\u001b[39m X, y = iris.data, iris.target\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m result = \u001b[43mbootstrap_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m method, stats \u001b[38;5;129;01min\u001b[39;00m result.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mbootstrap_clustering\u001b[39m\u001b[34m(X, y, n_clusters, n_iter)\u001b[39m\n\u001b[32m     92\u001b[39m fcm = FCM(n_clusters=n_clusters)\n\u001b[32m     93\u001b[39m fcm.fit(X_sample)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m pc, xbi, fsi = \u001b[43mfuzzy_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m labels = fcm.u.T.argmax(axis=\u001b[32m0\u001b[39m)\n\u001b[32m     96\u001b[39m ss = silhouette_score(X_sample, labels)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mfuzzy_indices\u001b[39m\u001b[34m(X, membership, centers, m)\u001b[39m\n\u001b[32m     17\u001b[39m pc = np.sum(membership ** \u001b[32m2\u001b[39m) / n_samples\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Compactness (for XBI)\u001b[39;00m\n\u001b[32m     20\u001b[39m compactness = np.sum([\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     np.sum(\u001b[43m(\u001b[49m\u001b[43mmembership\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(centers.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     23\u001b[39m ])\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Separation for XBI\u001b[39;00m\n\u001b[32m     26\u001b[39m sep = np.min([\n\u001b[32m     27\u001b[39m     np.linalg.norm(centers[i] - centers[j]) ** \u001b[32m2\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(centers)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i + \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(centers))\n\u001b[32m     29\u001b[39m ])\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (3,) (150,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_completeness_v_measure,\n",
    ")\n",
    "from fcmeans import FCM\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fuzzy_indices(X, membership, centers, m=2):\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "\n",
    "    # Compactness (for XBI)\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[:, k] ** m) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(centers.shape[0])\n",
    "    ])\n",
    "\n",
    "    # Separation for XBI\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "\n",
    "    # FS Index\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[:, k] ** m) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    return pc, xbi, fsi\n",
    "\n",
    "def hard_indices(X, labels, true_labels):\n",
    "    n_samples = X.shape[0]\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2 or -1 in unique_labels:\n",
    "        return [np.nan] * 9\n",
    "\n",
    "    centers = np.array([X[labels == label].mean(axis=0) for label in unique_labels])\n",
    "    membership = np.zeros((n_samples, len(unique_labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        membership[i, np.where(unique_labels == label)[0][0]] = 1\n",
    "\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[:, k] ** 2) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(len(unique_labels))\n",
    "    ])\n",
    "\n",
    "    sep = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(len(centers)) for j in range(i + 1, len(centers))\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * sep) if sep > 0 else np.inf\n",
    "\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "    separation = np.sum([\n",
    "        np.sum(membership[:, k] ** 2) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(len(centers))\n",
    "    ])\n",
    "    fsi = compactness - separation\n",
    "\n",
    "    ss = silhouette_score(X, labels)\n",
    "    ars = adjusted_rand_score(true_labels, labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, labels)\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_labels, labels)\n",
    "\n",
    "    return pc, xbi, fsi, ss, ars, ami, h, c, v\n",
    "\n",
    "def bootstrap_clustering(X, y, n_clusters=3, n_iter=100):\n",
    "    results = {name: [] for name in ['KMeans', 'FCM', 'GK', 'DBSCAN', 'Spectral', 'Hierarchical']}\n",
    "\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        idx = np.random.choice(len(X), len(X), replace=True)\n",
    "        X_sample = X[idx]\n",
    "        y_sample = y[idx]\n",
    "\n",
    "        # KMeans\n",
    "        km = KMeans(n_clusters=n_clusters, n_init=10).fit(X_sample)\n",
    "        results['KMeans'].append(hard_indices(X_sample, km.labels_, y_sample))\n",
    "\n",
    "        # FCM\n",
    "        fcm = FCM(n_clusters=n_clusters)\n",
    "        fcm.fit(X_sample)\n",
    "        pc, xbi, fsi = fuzzy_indices(X_sample, fcm.u.T, fcm.centers)\n",
    "        labels = fcm.u.T.argmax(axis=0)\n",
    "        ss = silhouette_score(X_sample, labels)\n",
    "        ars = adjusted_rand_score(y_sample, labels)\n",
    "        ami = adjusted_mutual_info_score(y_sample, labels)\n",
    "        h, c, v = homogeneity_completeness_v_measure(y_sample, labels)\n",
    "        results['FCM'].append((pc, xbi, fsi, ss, ars, ami, h, c, v))\n",
    "\n",
    "        # GK (not implemented; placeholder)\n",
    "        results['GK'].append((np.nan,) * 9)\n",
    "\n",
    "        # DBSCAN\n",
    "        db = DBSCAN(eps=0.5, min_samples=5).fit(X_sample)\n",
    "        results['DBSCAN'].append(hard_indices(X_sample, db.labels_, y_sample))\n",
    "\n",
    "        # Spectral Clustering\n",
    "        sc = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors').fit(X_sample)\n",
    "        results['Spectral'].append(hard_indices(X_sample, sc.labels_, y_sample))\n",
    "\n",
    "        # Hierarchical\n",
    "        ac = AgglomerativeClustering(n_clusters=n_clusters).fit(X_sample)\n",
    "        results['Hierarchical'].append(hard_indices(X_sample, ac.labels_, y_sample))\n",
    "\n",
    "    # Summarize results\n",
    "    summary = {}\n",
    "    for method, scores in results.items():\n",
    "        scores = np.array(scores, dtype=np.float64)\n",
    "        summary[method] = {\n",
    "            'mean': np.nanmean(scores, axis=0).round(4),\n",
    "            'std': np.nanstd(scores, axis=0).round(4),\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "# Run the evaluation\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "result = bootstrap_clustering(X, y, n_clusters=3, n_iter=100)\n",
    "\n",
    "# Print summary\n",
    "for method, stats in result.items():\n",
    "    print(f\"\\n{method}\")\n",
    "    print(\"Mean:\", stats['mean'])\n",
    "    print(\"Std :\", stats['std'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_indices(X, membership, centers, m=2.0):\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = centers.shape[0]\n",
    "\n",
    "    # Partition Coefficient (PC)\n",
    "    pc = np.sum(membership ** 2) / n_samples\n",
    "\n",
    "    # Compactness (for XBI)\n",
    "    compactness = np.sum([\n",
    "        np.sum((membership[:, k] ** m) * np.linalg.norm(X - centers[k], axis=1) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "\n",
    "    # Separation (for XBI)\n",
    "    separation = np.min([\n",
    "        np.linalg.norm(centers[i] - centers[j]) ** 2\n",
    "        for i in range(n_clusters)\n",
    "        for j in range(i + 1, n_clusters)\n",
    "    ])\n",
    "    xbi = compactness / (n_samples * separation) if separation > 0 else np.inf\n",
    "\n",
    "    # Global centroid for FSI\n",
    "    global_centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Separation part for FSI\n",
    "    sep_fsi = np.sum([\n",
    "        np.sum(membership[:, k] ** m) * np.linalg.norm(centers[k] - global_centroid) ** 2\n",
    "        for k in range(n_clusters)\n",
    "    ])\n",
    "\n",
    "    fsi = compactness - sep_fsi\n",
    "\n",
    "    return round(pc, 4), round(xbi, 4), round(fsi, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
